<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>U-Net</title>
    <url>/2025/01/12/U-Net/</url>
    <content><![CDATA[<h1 id="U-Net"><a href="#U-Net" class="headerlink" title="U-Net"></a>U-Net</h1><p>​<br>The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization.</p>
<p>该架构由一个捕捉上下文的收缩路径和一个实现精确定位的对称扩展路径组成</p>
 <span id="more"></span>
<p><strong>Hence, Ciresan et al.trained a network in a sliding-window setup to predict the class label of each pixel by providing a local region (patch) around that pixel as input.</strong></p>
<p><strong>This network can localize.</strong></p>
<p><strong>Two drawbacks: slow and redundant ,trade-off between localization accuracy and the use of context.</strong></p>
<p>The main idea in fully convolutional network is to supplement a usual contracting network by successive layers, where pooling operators are replaced by upsampling operators.</p>
<p>全卷积网络的主要思想是通过<strong>连续的层来补充通常的收缩网络，其中池化算子被上采样算子所取代。</strong></p>
<p>FCN的主要工作是将传统CNN后面的全连接层换成了卷积层，这样网络的输出将是热力图而非类别；同时，为解决卷积和池化导致图像尺寸的变小，使用上采样方式对图像尺寸进行恢复，并且提出一个跳级结构，让pool4和pool3之后的特征图于conv7之后的特征图相加，得到新的更加准确的分割图像。</p>
<p>这些layers增加了输出的分辨率。为了定位，通过收缩路径(conracting path)的高分辨率特征图和上采样的输出相结合，之后的连续卷积层可以来通过这些信息来学习更加精准的输出。</p>
<p><strong>对于上采样保留了很多的特征通道，这个网络没有全连接层，只有卷积层。</strong></p>
<p>只包含卷积就保证了seamless segmentation of arbitrarily large images by an overlap-tile strategy。通过重叠瓦片技术对任意大图形进行无缝分割。通过mirroring the input image.</p>
<p>To predict the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image. This tiling strategy is important to apply the network to large images, since otherwise the resolution would be limited by the GPU memory.</p>
<p><strong>针对于数据增广，文章使用了弹性变换的方式，使得网络能够学习对这种变形的不变性，而不需要在注释的图像语料库中看到这些转换。</strong></p>
<p>对同类型的细胞的分离是很打的一个挑战。文章使用了weighted loss，针对不同地区进行不同的loss学习，让网络对边缘(border)进行学习，达到分离的效果。</p>
<p>网络结构：</p>
<p>左边：contracting path(收缩路径) 右边：expansive path(扩展路径)</p>
<p>Contracting path :重复使用2个没有padding的3<em>3卷积，每个卷积之后包含ReLU和一个2</em>2最大池化（stride=2），同时，在每次下采样的过程中将通道数*2.</p>
<p>Expansive path : 2<em>2卷积进行上采样同时将通道数/2， 与收缩路径中相应裁剪的特征图相连接，两个3</em>3卷积每个都跟着一个ReLU。Cropping是重要的因为在卷积操作中会造成边缘像素的丢失。</p>
<p>最后一层有一个1*1的卷积将64个特征向量映射到每个所期待的类别。</p>
<p>总共有23层。</p>
<p>为了允许输出分割图的无缝平铺（见图2），重要的是选择输入平铺的大小，使所有2x2的最大池化操作都应用于一个具有偶数x和y大小的层。（换句话说，contracting path中经过卷积和ReLU之后的图像的x，y都是偶数）</p>
<p>训练：</p>
<p>To minimize the overhead and make maximum use of the GPU memory, we favor large input tiles over a large batch size and hence reduce the batch to a single image.</p>
<p>为了最大限度地减少开销和最大限度地利用GPU内存，我们倾向于使用大的输入瓦片而不是大的批处理量，从而将批处理量减少到单一图像。（换句话说就是图像大，batch size小）</p>
<p>Accordingly we use a high momentum (0.99) such that a large number of the previously seen training samples determine the update in the current optimization step.</p>
<p>因此，我们使用一个高动量（0.99），这样大量的先前看到的训练样本决定了当前优化步骤中的更新。</p>
<p>能量函数(energy function)一开始在热力学中被定义，用于描述系统的能量值，当能量值达到最小时系统达到稳定状态。（换句话说就是优化函数）</p>
<p>energy function： pixel-wise soft-max + Cross entropy</p>
<p>Weighted map：</p>
<p>需要有一个好的初始化a good initialization of the weights is extremely important</p>
<p>Ideally the initial weights should be adapted such that each feature map in the network has approximately unit variance. 理想情况下，初始权重应该被调整，使网络中的每个特征图具有近似的单位方差。</p>
<p>可以通过一个标准差为  的高斯分布得到。</p>
<p>Data augmentation</p>
<p>random elastic deformations is key concept..</p>
<p>We generate smooth deformations using random displacement vectors on a coarse 3 by 3 grid. The displacements are sampled from a Gaussian distribution with 10 pixels standard deviation. Per-pixel displacements are then computed using bicubic interpolation. Drop-out layers at the end of the contracting path perform further implicit data augmentation</p>
<p>我们在一个粗略的3乘3的网格上使用随机位移矢量生成平滑的变形。位移是从标准偏差为10像素的高斯分布中采样的。然后使用双三次插值计算每像素的位移。在收缩路径末端的剔除层进行进一步的隐性数据增强</p>
<p>Reference:</p>
<p><a href="https://blog.csdn.net/qq_36717487/article/details/115368483">语义分割与实例分割的区别_实例分割和语义分割-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/lsb2002/article/details/132005658">机器学习：监督学习、无监督学习、半监督学习、强化学习-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/qq_41731861/article/details/120511148">FCN（全卷积神经网络）详解-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/maliang_1993/article/details/82020596">仿射和弹性变换（affine and elastic transform）的python实现_图像增强弹性变换-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/u011622208/article/details/112566676">【数据增强】——弹性变形(Elastic Distortion)_数据增强随机弹性变换-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/weixin_36474809/article/details/87931260">Unet论文详解U-Net:Convolutional Networks for Biomedical Image Segmentation-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/gaoxueyi551/article/details/105238182">深度学习中的Momentum算法原理-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/qq_43374104/article/details/106434440">梯度下降优化算法Momentum_momentum梯度下降法-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/hejunqing14/article/details/50057001">能量函数在神经网络中的意义_energy function-CSDN博客</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>论文学习</tag>
      </tags>
  </entry>
  <entry>
    <title>u-kan</title>
    <url>/2025/01/12/u-kan/</url>
    <content><![CDATA[<p><a href="https://blog.csdn.net/jarodyv/article/details/138751803">深入理解 Kolmogorov–Arnold Networks (KAN)_kan: kolmogorov–arnold networks-CSDN博客</a></p>
<p><a href="https://www.zhihu.com/question/654782350">(5 封私信 / 14 条消息) 如何评价神经网络架构KAN，是否有潜力取代MLP？ - 知乎 (zhihu.com)</a></p>
<p><a href="https://blog.csdn.net/fg13821267836/article/details/93405572">多层感知机（MLP）简介-CSDN博客</a></p>
<p>MLP说白了就是最简单的<script type="math/tex">f(w_i×x+b_i)+softmax</script>.</p>
<p><a href="https://blog.csdn.net/qq_46703208/article/details/130539464">深度学习中的token_深度学习token-CSDN博客</a></p>
<p>token:最小的单元。在nlp领域，就是一个单词。cv领域，在vit中就是16x16的patch</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>论文学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Temperature scaling</title>
    <url>/2025/01/12/Temperature-scaling/</url>
    <content><![CDATA[<p>Temperature scaling</p>
<p>后验概率被应用在训练过程之后目的是学习一个重新校准的函数。为了达到这个目的，训练集的一部分被拿出作为校准集。重新校准函数适用于网络的输出（如logit向量），并产生一个改进的校准，该校准是在遗漏的校准集上学习的。<br><span id="more"></span><br>Temperature scaling是data-efficiency的accuracy-preserving方法，但是具有稍微不太好的expressive</p>
<p>Selective Scaling：</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image002-1736691287914-3.png)</p>
<p>原始：</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image004-1736691287914-4.png)<img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image006-1736691287914-2.png)</p>
<p>只有卷积：<img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image007.png)</p>
<p>只有池化：</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image009.png)</p>
<p>只有池化：</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image011.png)<img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image013.png)</p>
<p>1.5</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image015.png)</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image017-1736691287913-1.png)</p>
<p>1.6：</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image019-1736691287914-5.png)<img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image021.png)</p>
<p>2.0：</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image023.png)<img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image025.png)</p>
<p>1.8+ece做loss</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image027.png)<img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image029.png)</p>
<p>1.8+ece做loss+池化</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image031-1736691287914-6.png)<img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image033.png)</p>
<p>Reference：</p>
<p><a href="https://blog.csdn.net/m0_51233386/article/details/128414228">Ubuntu下tar命令使用详解 .tar解压、.tar压缩_ubuntu解压tar.gz-CSDN博客</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/630739668">Logistic（逻辑斯蒂）函数浅谈 - 知乎 (zhihu.com)</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/335765830">简介欧拉第一类积分：Beta函数 - 知乎 (zhihu.com)</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/63184325">神经网络1：多层感知器-MLP - 知乎 (zhihu.com)</a></p>
<p><a href="https://blog.csdn.net/lingzhou33/article/details/87901365">语义分割的评价指标——IoU_语义分割iou-CSDN博客</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/33841176">CNN 入门讲解：什么是全连接层（Fully Connected Layer）? - 知乎 (zhihu.com)</a></p>
<p><a href="https://blog.csdn.net/qq_37369201/article/details/109195257">python中assert的用法（简洁明了）_assert python-CSDN博客</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/666048268">Expected Calibration Error (ECE) - 知乎 (zhihu.com)</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>校准</tag>
      </tags>
  </entry>
  <entry>
    <title>PVT encoder</title>
    <url>/2025/01/12/PVT-encoder/</url>
    <content><![CDATA[<h1 id="PVT-encoder"><a href="#PVT-encoder" class="headerlink" title="PVT encoder"></a>PVT encoder</h1><p>use of transformer</p>
<p>For example, some works model the vision task as a dictionary lookup problem with learnable queries, and use the Transformer decoder as a task-specific head on top of the CNN backbone.</p>
<p>将视觉任务建模为一个具有可学习查询的字典查询问题，并将Transformer解码器作为CNN主干之上的特定任务头。</p>
<span id="more"></span>
<h2 id="ViT"><a href="#ViT" class="headerlink" title="ViT"></a>ViT</h2><p>拥有圆柱形的粗略images patches as input </p>
<p>shortcomings：</p>
<ol>
<li>输出特征图是单尺度而且底分辨率</li>
<li>计算花销大</li>
</ol>
<h2 id="PVT"><a href="#PVT" class="headerlink" title="PVT"></a>PVT</h2><p>a pure transformer backbone </p>
<p>advantages:</p>
<ol>
<li>细粒度(fine-grained)的图像patches 来学习高分辨率表征</li>
<li>引入一个逐渐减小的金字塔transformer的序列长度，减少计算量</li>
<li>引入空间降低注意力层(spatial-reduce attention layer)降低在学习高分辨率特征图的开销</li>
<li>产生全局感受野(global reception field)</li>
</ol>
<p><img src="D:\Program Files (x86" alt="image-20240812121219476">\Typora\img-data\image-20240812121219476.png)</p>
<p>Feature Pyramid for Transformer</p>
<p>首先将输入特征图分为<script type="math/tex">\frac{H_i W_i}{P_i^2}</script>个patches，将每个patches展平之后并且将其映射到(projected to)<script type="math/tex">C_i</script>维度的embedding，再经过一个线性映射之后，embedded patches可以被视为<script type="math/tex">\frac{H_i-1}{P_i}\times \frac{W_i-1}{P_i} \times C_i</script></p>
<h2 id="SRA-？"><a href="#SRA-？" class="headerlink" title="SRA ？"></a>SRA ？</h2><p><img src="D:\Program Files (x86" alt="image-20240812152215411">\Typora\img-data\image-20240812152215411.png)</p>
<p>和MHA(multi-head attention)相似，receive Q K V,它在进行注意力机制之前减少了K和V的空间尺度，显著降低了计算和存贮</p>
<p><img src="D:\Program Files (x86" alt="image-20240814091739353">\Typora\img-data\image-20240814091739353.png)</p>
<p>这里和原始transformer一样，只不过K和V先经过了一个空间降低操作。</p>
<p><img src="D:\Program Files (x86" alt="image-20240814091832785">\Typora\img-data\image-20240814091832785.png)</p>
<p><img src="D:\Program Files (x86" alt="image-20240814091918147">\Typora\img-data\image-20240814091918147.png)</p>
<p>hyper parameters:</p>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.192ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 969 840.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mi" transform="translate(675,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>: patch size of Stage <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container></p>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.357ex" height="1.952ex" role="img" focusable="false" viewBox="0 -705 1042 862.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="mi" transform="translate(748,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>: channel number of the output of Stage <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container></p>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.28ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 1008 840.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(714,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>: encoder layers number in Stage <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container></p>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.457ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 1086 840.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="mi" transform="translate(792,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>: reduction ratio of the SRA in Stage <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container></p>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.556ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 1130 840.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mi" transform="translate(836,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>: head number of the SRA in Stage <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container></p>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.409ex" height="1.895ex" role="img" focusable="false" viewBox="0 -680 1065 837.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="mi" transform="translate(771,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>: expansion ratio of the feed-forward layer in Stage <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container></p>
<p><strong>the rule of ResNet:</strong></p>
<ol>
<li><strong>use small output channel numbers in shallow stages</strong></li>
<li><strong>concentrate the major computation resource in intermediate stages.</strong></li>
<li><strong>with the growth of network depth, the hidden dimension gradually increases, and the output resolution progressively shrinks</strong></li>
<li><strong>the major computation resource is concentrated in Stage 3</strong></li>
</ol>
<p>the advantages over ViT:</p>
<ol>
<li>more flexible—can generate feature maps of different scales/channels in different stages</li>
<li>more versatile(通用)—can be easily plugged and played in most downstream task models</li>
<li>more friendly to computation/memory—can handle higher resolution feature maps or longer sequences</li>
</ol>
<p>Reference:</p>
<p><a href="https://blog.csdn.net/qq_38890412/article/details/120601834">全网最通俗易懂的 Self-Attention自注意力机制 讲解_self attention机制-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/weixin_42392454/article/details/122478544">狗都能看懂的Self-Attention讲解-CSDN博客</a></p>
<p><a href="https://www.bilibili.com/video/BV15v411W78M/?spm_id_from=333.788">https://www.bilibili.com/video/BV15v411W78M/?spm_id_from=333.788</a></p>
<p><a href="https://blog.csdn.net/oYeZhou/article/details/114288247">Pyramid Vision Transformer（PVT）: 纯Transformer设计，用于密集预测的通用backbone-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/weixin_73179708/article/details/132516512">从零开始了解transformer的机制|第四章：FFN层的作用-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/weixin_51756104/article/details/127250190">对Transformer中FeedForward层的理解_feedforward层的作用-CSDN博客</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>论文学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Mask-TS and ODC-SA net</title>
    <url>/2025/01/12/Mask-TS-and-ODC-SA-net/</url>
    <content><![CDATA[<h1 id="Mask-TS"><a href="#Mask-TS" class="headerlink" title="Mask-TS"></a>Mask-TS</h1><p><img src="D:\Program Files (x86" alt="image-20240705115352035">\Typora\img-data\image-20240705115352035.png)</p>
<p>将原图、模型输出logits、概率图、不确定性图分别输入到4个一样的卷积网络当中，4个通过卷积之后代表它们考虑了像素点之间的位置关系。<strong>（只要通过了卷积网络就已经考虑了像素之间的位置关系）</strong>这四个卷积的输出放入一个注意力通道当中，通过自适应权重进行相加。</p>
<p>Hi和Pi这两个分支分别在增强边缘形状和更仔细地处理预测的真值方面发挥了作用，而x i包含丰富的原始信息，z i拥有与分割有关的特征，因为它源于一个分割网络</p>
<p>被认为是背景的部分就保持原始全局T0不变，被认为是病变区域的就用校准之后pre-output T‘。<br><span id="more"></span></p>
<h1 id="Mask-loss"><a href="#Mask-loss" class="headerlink" title="Mask-loss"></a>Mask-loss</h1><p><img src="D:\Program Files (x86" alt="image-20240705183615243">\Typora\img-data\image-20240705183615243.png)</p>
<p><img src="D:\Program Files (x86" alt="image-20240705183653977">\Typora\img-data\image-20240705183653977.png)</p>
<p>先将label和prediction合并，然后将calibrated logits的背景视为白色，忽略掉背景，将prediction黑白反转。再将两图合并，得到一个关注边缘的图像。再利用的二分类交叉熵损失函数。</p>
<h2 id="confidence-and-accuracy"><a href="#confidence-and-accuracy" class="headerlink" title="confidence and accuracy"></a>confidence and accuracy</h2><p>In learning algorithm, Confidence defines the probability of the event (or probability of input to fall in different classes). </p>
<p><strong>If a class has high probability then it has high confidence</strong>. Confidence value can be calculated for single input as well giving the meaning as how much the algorithm is confident for that class.</p>
<p>On the other hand, <strong>accuracy defines the skill of the learning algorithm to predict accurately.</strong> It defines the percentage of correct predictions made from all predictions.</p>
<h2 id="ECE是越小越好"><a href="#ECE是越小越好" class="headerlink" title="ECE是越小越好"></a>ECE是越小越好</h2><h1 id="ODC-SA-Net"><a href="#ODC-SA-Net" class="headerlink" title="ODC-SA Net"></a>ODC-SA Net</h1><p>to deal with multi-directional features and drastic changes in scale.多方向的特征和规模的急剧变化。</p>
<h2 id="existing-problems"><a href="#existing-problems" class="headerlink" title="existing problems"></a>existing problems</h2><ol>
<li><p>In the target images containing extremely hidden polyps, the angles of them change randomly with the movement of the lens, which makes the features of the polyps no longer representative in a fixed or specific few directions.在含有极度隐藏的息肉的目标图像中，它们的角度随着镜头的移动而随机变化，这使得息肉的特征不再代表固定或特定的几个方向。</p>
</li>
<li><p>The size of the hidden object in the polyp image varies dramatically息肉图像中隐藏物体的大小变化很大</p>
</li>
<li><p>Most of the existing methods ignore the importance of feature reorganization after the multi-layer feature fusion operation for the segmentation problem, which makes the rich semantic information obtained by the encoder cannot be fully utilized.现有的方法大多忽略了多层特征融合操作后的特征重组对分割问题的重要性，这使得编码器获得的丰富语义信息不能得到充分的利用。</p>
</li>
</ol>
<p><img src="D:\Program Files (x86" alt="image-20240714165544654">\Typora\img-data\image-20240714165544654.png)</p>
<h2 id="ODC：Orthogonal-Direction-Enhancement-正交方向增强"><a href="#ODC：Orthogonal-Direction-Enhancement-正交方向增强" class="headerlink" title="ODC：Orthogonal Direction Enhancement  正交方向增强"></a>ODC：Orthogonal Direction Enhancement  正交方向增强</h2><blockquote>
<p>正交方向卷积（ODC）块可以通过形成正交特征向量基础，利用转置的矩形卷积核提取多方向特征，解决了随机特征方向变化的问题，减少了计算负荷。</p>
</blockquote>
<p><img src="D:\Program Files (x86" alt="image-20240715142552391">\Typora\img-data\image-20240715142552391.png)</p>
<h3 id="需要解决的问题："><a href="#需要解决的问题：" class="headerlink" title="需要解决的问题："></a>需要解决的问题：</h3><p>different location and shooting direction of the colonoscopy-lens, the usual convolution kernel will miss features in specific directions due to the rotation of the image or the target because of its square shape and fixed sliding direction</p>
<p>由于结肠镜-透镜的不同位置和拍摄方向，通常的卷积核会因为图像或目标的旋转而错过特定方向的特征，因为它是方形的，而且滑动方向固定。</p>
<p>solves the problem of random changes in target feature directions？</p>
<h3 id="解决方法："><a href="#解决方法：" class="headerlink" title="解决方法："></a>解决方法：</h3><p>一对转置矩形卷积核（1×3和3×1）分别提取最深层图像的每个通道的垂直和水平的特征来形成一个正交特征基准</p>
<p>每个通道特征可以用一个线性表达式来表示:</p>
<script type="math/tex; mode=display">
\pmb{f}^k = a_k\pmb{r}^k + b_k\pmb{c}^k = a_kr^k\pmb{i} + b_kc^k\pmb{j}</script><p>网络下一层的任何通道的特征图应该是上一层的所有通道与任何方向的特征的组合</p>
<p><img src="D:\Program Files (x86" alt="image-20240715142428834">\Typora\img-data\image-20240715142428834.png)</p>
<p>具体操作：</p>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.661ex;" xmlns="http://www.w3.org/2000/svg" width="15.208ex" height="2.891ex" role="img" focusable="false" viewBox="0 -985.6 6722 1277.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path></g><g data-mml-node="TeXAtom" transform="translate(837.3,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(676,-284.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1518.6,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="msup" transform="translate(2463.4,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(755,365.5) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mfrac"><g data-mml-node="mi" transform="translate(259.6,394) scale(0.707)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mn" transform="translate(220,-345) scale(0.707)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)"></path></g><rect width="907.1" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(1147.1,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mfrac" transform="translate(1925.1,0)"><g data-mml-node="mi" transform="translate(220,394) scale(0.707)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mn" transform="translate(237,-345) scale(0.707)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)"></path></g><rect width="941" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(3106.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mn" transform="translate(3884.2,0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)"></path></g></g></g></g></g></svg></mjx-container>  </p>
<p> <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="50.185ex" height="2.587ex" role="img" focusable="false" viewBox="0 -985.6 22181.7 1143.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mpadded"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mspace"></g><g data-mml-node="msub" transform="translate(55.3,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(484,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1111,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="msup" transform="translate(2055.8,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(755,365.5) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mfrac"><g data-mml-node="mi" transform="translate(259.6,394) scale(0.707)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mn" transform="translate(220,-345) scale(0.707)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)"></path></g><rect width="907.1" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(1147.1,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mfrac" transform="translate(1925.1,0)"><g data-mml-node="mi" transform="translate(220,394) scale(0.707)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mn" transform="translate(237,-345) scale(0.707)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)"></path></g><rect width="941" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(3106.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mn" transform="translate(3884.2,0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)"></path></g></g></g><g data-mml-node="mo" transform="translate(6536.6,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(7536.8,0)"><g data-mml-node="mpadded"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g></g></g><g data-mml-node="mspace" transform="translate(7536.8,0)"></g><g data-mml-node="msub" transform="translate(7592.1,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(466,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(8629.8,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="msup" transform="translate(9574.6,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(755,365.5) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mfrac"><g data-mml-node="mi" transform="translate(259.6,394) scale(0.707)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mn" transform="translate(220,-345) scale(0.707)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)"></path></g><rect width="907.1" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(1147.1,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mfrac" transform="translate(1925.1,0)"><g data-mml-node="mi" transform="translate(220,394) scale(0.707)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mn" transform="translate(237,-345) scale(0.707)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)"></path></g><rect width="941" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(3106.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mn" transform="translate(3884.2,0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)"></path></g></g></g><g data-mml-node="mo" transform="translate(14111,0)"><path data-c="2192" d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(15388.8,0)"><g data-mml-node="mpadded"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g></g></g><g data-mml-node="mspace" transform="translate(15388.8,0)"></g><g data-mml-node="msub" transform="translate(15444,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(609,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(16624.7,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="msup" transform="translate(17569.5,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(755,365.5) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mfrac"><g data-mml-node="mi" transform="translate(259.6,394) scale(0.707)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mn" transform="translate(220,-345) scale(0.707)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)"></path></g><rect width="907.1" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(1147.1,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mfrac" transform="translate(1925.1,0)"><g data-mml-node="mi" transform="translate(220,394) scale(0.707)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mn" transform="translate(237,-345) scale(0.707)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)"></path></g><rect width="941" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(3106.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mn" transform="translate(3884.2,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)"></path><path data-c="38" d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z" transform="translate(1000,0)"></path></g></g></g></g></g></svg></mjx-container>   ?为啥（我只能理解为它用了padding）</p>
<p>一个三层的1×3卷积去提取水平特征<script type="math/tex">r_i</script> 另一个三层的3×1的卷积去提取垂直特征<script type="math/tex">c_i</script></p>
<p>然后将这两个特征通过线性组合合并得到<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.043ex" height="1.927ex" role="img" focusable="false" viewBox="0 -694 903 851.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mi" transform="translate(609,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container></p>
<p>最后再进行一个残差连接，把原图接进来，得到<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="15.055ex" height="2.669ex" role="img" focusable="false" viewBox="0 -985.6 6654.4 1179.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mpadded"><g data-mml-node="mi"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g></g></g><g data-mml-node="mspace"></g><g data-mml-node="msub" transform="translate(55.3,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(824,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1451,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="msup" transform="translate(2395.8,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(755,365.5) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mfrac"><g data-mml-node="mi" transform="translate(259.6,394) scale(0.707)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mn" transform="translate(220,-345) scale(0.707)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)"></path></g><rect width="907.1" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(1147.1,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mfrac" transform="translate(1925.1,0)"><g data-mml-node="mi" transform="translate(220,394) scale(0.707)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mn" transform="translate(237,-345) scale(0.707)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)"></path></g><rect width="941" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(3106.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mn" transform="translate(3884.2,0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)"></path></g></g></g></g></g></svg></mjx-container></p>
<p>​    </p>
<h2 id="SA-Scale-Aware-扩大感受野-MSFA"><a href="#SA-Scale-Aware-扩大感受野-MSFA" class="headerlink" title="SA: Scale Aware  扩大感受野 MSFA"></a>SA: Scale Aware  扩大感受野 MSFA</h2><blockquote>
<p>Multi-scale Fusion Attention mechanism  多尺度融合注意（MSFA）机制 </p>
<p>emphasize scale changes in both spatial and channel dimensions, enhancing the segmentation accuracy for polyps of varying size 强调了空间和通道维度上的尺度变化，提高了不同大小的息肉的分割精度。</p>
</blockquote>
<p><img src="D:\Program Files (x86" alt="image-20240724171951166">\Typora\img-data\image-20240724171951166.png)</p>
<h3 id="需要解决的问题：-1"><a href="#需要解决的问题：-1" class="headerlink" title="需要解决的问题："></a>需要解决的问题：</h3><p>The growth time of polyps in the intestine, the environment and the shooting distance of the colonoscopy lens are different. The polyps contained in the pictures have different sizes, and their color and texture characteristics are very similar to the background.</p>
<p>息肉在肠道、环境中的生长时间和结肠镜镜的拍摄距离是不同的。图片中包含的息肉有不同的尺寸，它们的颜色和纹理特征与背景非常相似。</p>
<p>Most of the existing methods treat scale-varying features as the same and ignore the attention bias caused by their differences in spatial and channel dimensions.</p>
<p>大多数现有的方法将标度变化的特征视为相同，而忽略了由它们在空间和通道维度上的差异引起的注意力偏差。</p>
<h3 id="解决方法：-1"><a href="#解决方法：-1" class="headerlink" title="解决方法："></a>解决方法：</h3><p>提出Spatial Scalable Enhancement Mechanism (S2E) and Channel Scalable Attention Mechanism (CSA)</p>
<p>S2E增强多尺度的空间特征，CSA在通道维度增强多尺度注意力</p>
<p>expand the receptive field in both spatial and channel dimensions by convolutional and pooling parallel dual channels of the high-level feature map Q i containing more semantic information, and use the feature map of the lower layer x3 i , which has more detail information than Q i , to give attention to the deep semantics of objects with different sizes to de-emphasize the influence of background and interference</p>
<p>通过卷积和池化并行的双通道方式扩大高等级特征图<script type="math/tex">Q_i</script>在空间和通道两个维度上的感受野，并使用包含更多详细信息的底层特征的<script type="math/tex">x^3_i</script>增加对于深层语义分割不同大小的注意力，降低对背景的关注。</p>
<h3 id="S2E：Spatial-Scalable-Enhancement-Mechanism-空间可扩展的增强机制"><a href="#S2E：Spatial-Scalable-Enhancement-Mechanism-空间可扩展的增强机制" class="headerlink" title="S2E：Spatial Scalable Enhancement Mechanism 空间可扩展的增强机制"></a>S2E：Spatial Scalable Enhancement Mechanism 空间可扩展的增强机制</h3><p><img src="D:\Program Files (x86" alt="image-20240724180802044">\Typora\img-data\image-20240724180802044.png)</p>
<p>双线并行</p>
<p>一个是包含3个3*3的卷积层，可以获取局部特征信息并通过卷积操作增强其对于小物体的特征信息。</p>
<p>另一个是一个平均池化层，在一些特定区域（2*2大小），平均池化可以保留大目标，减弱小目标（由于更大的目标包含更多的像素点），这样之后新特征图的总体分布和大目标的分布更接近，同时也获得了全局的上下文信息。</p>
<p>它减弱了空间分辨率但增强了全局上下文信息在没有增大计算量的前提下。</p>
<p>在平均池化之后，插值被放大（没有对小物体的削弱进行补偿）来保证和第一个通道上的图片大小一致，并且相加。</p>
<p>这样可以做到融合了空间多尺度信息，同时不用很大的计算量。</p>
<p>Output：<script type="math/tex">S_i</script></p>
<h3 id="CSA：Channel-Scalable-Attention-Mechanism-通道可扩展注意机制"><a href="#CSA：Channel-Scalable-Attention-Mechanism-通道可扩展注意机制" class="headerlink" title="CSA：Channel Scalable Attention Mechanism 通道可扩展注意机制"></a>CSA：Channel Scalable Attention Mechanism 通道可扩展注意机制</h3><p><img src="D:\Program Files (x86" alt="image-20240724220323241">\Typora\img-data\image-20240724220323241.png)</p>
<p>要解决的问题：Different channels contain different semantic information, and there are scale differences in the targets mapped by these channels.不同的通道包含不同的语义信息，这些通道映射的目标也存在规模差异。</p>
<p>对于小目标：传统的 Point-Wise-Conv + ReLU 提升注意力</p>
<p>对于大目标：先进行平均池化之后再进行 Point-Wise-Conv + ReLU 利用每个通道特征图的像素平均值，在全局信息的引导下获得通道注意力，增加了大尺度特征图的权重</p>
<p>两个分支的输出相加送入Sigmiod得到输出<script type="math/tex">W_i</script>，之后和<script type="math/tex">S_i</script>相点乘之后再和<script type="math/tex">Q_i</script>相加得到<script type="math/tex">D_i</script>   ?学长好像写错了，应该是先和Si点乘之后再和Qi相加</p>
<p>Output：<script type="math/tex">D_i</script></p>
<h3 id="RFA：Residual-Fusion-Attention-剩余融合注意"><a href="#RFA：Residual-Fusion-Attention-剩余融合注意" class="headerlink" title="RFA：Residual Fusion Attention  剩余融合注意"></a>RFA：Residual Fusion Attention  剩余融合注意</h3><p><img src="D:\Program Files (x86" alt="image-20240726223106998">\Typora\img-data\image-20240726223106998.png)</p>
<script type="math/tex; mode=display">F^3_i$$来指导$$D_i$$进行浅层和深层特征融合。



$$D_i$$经过上采样和驻点卷积使得和$$F^3_i$$大小一样，同样$$F^3_i$$也经过逐点卷积得到相同大小，相加之后经过一个ReLU激活函数和一个Conv block ，之后再进行一次残差连接，最后的通过softmax输出的$$E_i$$包含了每个像素的有效信息。最终$$E_i$$再和上采样之后的$$D_i$$点乘，自动关注目标区域。



Output:$$C_i</script><h2 id="Extraction-with-Re-attention-Modul-ERA"><a href="#Extraction-with-Re-attention-Modul-ERA" class="headerlink" title="Extraction with Re-attention Modul (ERA)"></a>Extraction with Re-attention Modul (ERA)</h2><blockquote>
<p>re-combine effective feature 重新组合有效的特征</p>
</blockquote>
<p>两组通道和空间注意机制用来提取经过多层特征融合之后的深度语义信息。特征重新提取和权重分配更好地利用语义信息并防止细节被忽略。</p>
<p><img src="D:\Program Files (x86" alt="image-20240727102332424">\Typora\img-data\image-20240727102332424.png)</p>
<p>further adjust and integrate its own feature information. This process generates a feature enhancement and combination graph that is more in line with the target. It allows different features to be combined into better features and can also realize the allocation of different feature emphasis degrees.</p>
<p>进一步调整和整合自己的特征信息。这个过程产生了一个更符合目标的特征增强和组合图。它允许不同的特征被组合成更好的特征，也可以实现不同特征强调程度的分配。</p>
<p>实现更好的拟合。</p>
<p>Output:<script type="math/tex">\widehat{Z_i}</script></p>
<h2 id="Shallow-Reverse-Attention-Mechanism-SRA"><a href="#Shallow-Reverse-Attention-Mechanism-SRA" class="headerlink" title="Shallow Reverse Attention Mechanism (SRA)"></a>Shallow Reverse Attention Mechanism (SRA)</h2><blockquote>
<p>enhance polyp edge with low level information 用低层次的信息增强息肉边缘</p>
</blockquote>
<p>渐层的编码输出有较多的边缘信息，指导RA（Reverse Attention）增强分割目标。</p>
<p><img src="D:\Program Files (x86" alt="image-20240727105341404">\Typora\img-data\image-20240727105341404.png)</p>
<p>problems:</p>
<p>the boundary between the background and the target is not obvious</p>
<p>solutions:</p>
<p>Inspired by ParNet, introduce RA (RA exploits the details of complementary regions by erasing the polyp regions that have been judged RA通过擦除已经判断过的息肉区域来利用互补区域的细节)</p>
<p>我们的网络使用最浅层的特征图来进行边界增强。</p>
<p>Output: <script type="math/tex">\hat{p_i}</script></p>
<h2 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h2><p><img src="D:\Program Files (x86" alt="image-20240727110943522">\Typora\img-data\image-20240727110943522.png)</p>
<p><img src="D:\Program Files (x86" alt="image-20240727110950672">\Typora\img-data\image-20240727110950672.png)</p>
<p>使用deep supervision <script type="math/tex">z_i</script>上采样到和<script type="math/tex">G</script>一样大，<script type="math/tex">\hat{p_i}</script>也一样。</p>
<h1 id="Reference："><a href="#Reference：" class="headerlink" title="Reference："></a>Reference：</h1><p><a href="https://blog.csdn.net/qq_39478403/article/details/121181904">【机器学习】详解 转置卷积 (Transpose Convolution)-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/yx868yx/article/details/107158692">增强感受野SPP、ASPP、RFB、PPM_spp模块的作用-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/weixin_39653948/article/details/104621249">markdown中的特殊字符、数学公式、图表等语法总结_markdown 制作流程图 有特殊符号-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/weixin_43964993/article/details/108360029#:~:text=加粗： %24pmb {字母}%24,加粗倾斜： %24boldsymbol {字母}%24">markdown公式中字母加粗_markdown 公式加粗-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/Ying_Xu/article/details/51240291">Latex所有常用数学符号整理_latex数学符号-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/tintinetmilou/article/details/81607721">Depthwise卷积与Pointwise卷积-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/m0_37605642/article/details/135958384">通俗易懂理解注意力机制(Attention Mechanism)-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/oYeZhou/article/details/114288247">Pyramid Vision Transformer（PVT）: 纯Transformer设计，用于密集预测的通用backbone-CSDN博客</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>论文学习</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode</title>
    <url>/2025/01/12/leetcode/</url>
    <content><![CDATA[<h1 id="Leetcode-704-二分查找-24-12-13"><a href="#Leetcode-704-二分查找-24-12-13" class="headerlink" title="Leetcode  704. 二分查找 24.12.13"></a>Leetcode  <a href="https://leetcode.cn/problems/binary-search/">704. 二分查找</a> 24.12.13</h1><h2 id="梦开始的地方"><a href="#梦开始的地方" class="headerlink" title="梦开始的地方"></a>梦开始的地方</h2><span id="more"></span>
<h2 id="题面"><a href="#题面" class="headerlink" title="题面"></a>题面</h2><p>给定一个 <code>n</code> 个元素有序的（升序）整型数组 <code>nums</code> 和一个目标值 <code>target</code> ，写一个函数搜索 <code>nums</code> 中的 <code>target</code>，如果目标值存在返回下标，否则返回 <code>-1</code>。</p>
<p><strong>示例 1:</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入: nums = [-1,0,3,5,9,12], target = 9</span><br><span class="line">输出: 4</span><br><span class="line">解释: 9 出现在 nums 中并且下标为 4</span><br></pre></td></tr></table></figure>
<p><strong>示例 2:</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入: nums = [-1,0,3,5,9,12], target = 2</span><br><span class="line">输出: -1</span><br><span class="line">解释: 2 不存在 nums 中因此返回 -1</span><br></pre></td></tr></table></figure>
<p><strong>提示：</strong></p>
<ol>
<li>你可以假设 <code>nums</code> 中的所有元素是不重复的。</li>
<li><code>n</code> 将在 <code>[1, 10000]</code>之间。</li>
<li><code>nums</code> 的每个元素都将在 <code>[-9999, 9999]</code>之间。</li>
</ol>
<h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">search</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> left=<span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> right=nums.<span class="built_in">size</span>()<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">while</span> (left&lt;=right)&#123;</span><br><span class="line">            <span class="type">int</span> middle=(left+right)/<span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span> (nums[middle]&lt;target)&#123;</span><br><span class="line">                left=middle<span class="number">+1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (nums[middle]&gt;target)&#123;</span><br><span class="line">                right=middle<span class="number">-1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">return</span> middle;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h1 id="Leetcode-27-移除元素-24-12-15"><a href="#Leetcode-27-移除元素-24-12-15" class="headerlink" title="Leetcode 27. 移除元素 24.12.15"></a>Leetcode <a href="https://leetcode.cn/problems/remove-element/">27. 移除元素</a> 24.12.15</h1><h2 id="题面-1"><a href="#题面-1" class="headerlink" title="题面"></a>题面</h2><p>给你一个数组 <code>nums</code> 和一个值 <code>val</code>，你需要 <strong><a href="https://baike.baidu.com/item/原地算法">原地</a></strong> 移除所有数值等于 <code>val</code> 的元素。元素的顺序可能发生改变。然后返回 <code>nums</code> 中与 <code>val</code> 不同的元素的数量。</p>
<p>假设 <code>nums</code> 中不等于 <code>val</code> 的元素数量为 <code>k</code>，要通过此题，您需要执行以下操作：</p>
<ul>
<li>更改 <code>nums</code> 数组，使 <code>nums</code> 的前 <code>k</code> 个元素包含不等于 <code>val</code> 的元素。<code>nums</code> 的其余元素和 <code>nums</code> 的大小并不重要。</li>
<li>返回 <code>k</code>。</li>
</ul>
<p><strong>用户评测：</strong></p>
<p>评测机将使用以下代码测试您的解决方案：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int[] nums = [...]; // 输入数组</span><br><span class="line">int val = ...; // 要移除的值</span><br><span class="line">int[] expectedNums = [...]; // 长度正确的预期答案。</span><br><span class="line">                            // 它以不等于 val 的值排序。</span><br><span class="line"></span><br><span class="line">int k = removeElement(nums, val); // 调用你的实现</span><br><span class="line"></span><br><span class="line">assert k == expectedNums.length;</span><br><span class="line">sort(nums, 0, k); // 排序 nums 的前 k 个元素</span><br><span class="line">for (int i = 0; i &lt; actualLength; i++) &#123;</span><br><span class="line">    assert nums[i] == expectedNums[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果所有的断言都通过，你的解决方案将会 <strong>通过</strong>。</p>
<p><strong>示例 1：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [3,2,2,3], val = 3</span><br><span class="line">输出：2, nums = [2,2,_,_]</span><br><span class="line">解释：你的函数函数应该返回 k = 2, 并且 nums 中的前两个元素均为 2。</span><br><span class="line">你在返回的 k 个元素之外留下了什么并不重要（因此它们并不计入评测）。</span><br></pre></td></tr></table></figure>
<p><strong>示例 2：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [0,1,2,2,3,0,4,2], val = 2</span><br><span class="line">输出：5, nums = [0,1,4,0,3,_,_,_]</span><br><span class="line">解释：你的函数应该返回 k = 5，并且 nums 中的前五个元素为 0,0,1,3,4。</span><br><span class="line">注意这五个元素可以任意顺序返回。</span><br><span class="line">你在返回的 k 个元素之外留下了什么并不重要（因此它们并不计入评测）。</span><br></pre></td></tr></table></figure>
<p><strong>提示：</strong></p>
<ul>
<li><code>0 &lt;= nums.length &lt;= 100</code></li>
<li><code>0 &lt;= nums[i] &lt;= 50</code></li>
<li><code>0 &lt;= val &lt;= 100</code></li>
</ul>
<h2 id="题解：-双指针"><a href="#题解：-双指针" class="headerlink" title="题解： 双指针"></a>题解： 双指针</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">removeElement</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> val)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> slow=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> fast=<span class="number">0</span>;fast&lt;nums.<span class="built_in">size</span>();fast++)&#123;</span><br><span class="line">            <span class="keyword">if</span> (nums[fast]!=val)&#123;</span><br><span class="line">                nums[slow++]=nums[fast];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> slow;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h1 id="Leetcode-977-有序数组的平方-24-12-15"><a href="#Leetcode-977-有序数组的平方-24-12-15" class="headerlink" title="Leetcode 977. 有序数组的平方 24.12.15"></a>Leetcode <a href="https://leetcode.cn/problems/squares-of-a-sorted-array/">977. 有序数组的平方</a> 24.12.15</h1><h2 id="题面-2"><a href="#题面-2" class="headerlink" title="题面"></a>题面</h2><p>给你一个按 <strong>非递减顺序</strong> 排序的整数数组 <code>nums</code>，返回 <strong>每个数字的平方</strong> 组成的新数组，要求也按 <strong>非递减顺序</strong> 排序。</p>
<p><strong>示例 1：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [-4,-1,0,3,10]</span><br><span class="line">输出：[0,1,9,16,100]</span><br><span class="line">解释：平方后，数组变为 [16,1,0,9,100]</span><br><span class="line">排序后，数组变为 [0,1,9,16,100]</span><br></pre></td></tr></table></figure>
<p><strong>示例 2：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [-7,-3,2,3,11]</span><br><span class="line">输出：[4,9,9,49,121]</span><br></pre></td></tr></table></figure>
<p><strong>提示：</strong></p>
<ul>
<li><code>1 &lt;= nums.length &lt;= 104</code></li>
<li><code>-104 &lt;= nums[i] &lt;= 104</code></li>
<li><code>nums</code> 已按 <strong>非递减顺序</strong> 排序</li>
</ul>
<p><strong>进阶：</strong></p>
<ul>
<li>请你设计时间复杂度为 <code>O(n)</code> 的算法解决本问题</li>
</ul>
<h2 id="题解：双指针，注意初始化vector长度"><a href="#题解：双指针，注意初始化vector长度" class="headerlink" title="题解：双指针，注意初始化vector长度"></a>题解：双指针，注意初始化vector长度</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">sortedSquares</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">res</span><span class="params">(nums.size())</span></span>;</span><br><span class="line">        <span class="type">int</span> t=nums.<span class="built_in">size</span>()<span class="number">-1</span>;</span><br><span class="line">        <span class="type">int</span> i,j;</span><br><span class="line">        <span class="keyword">for</span> (i=<span class="number">0</span>, j=nums.<span class="built_in">size</span>()<span class="number">-1</span>;i&lt;=j;)&#123;</span><br><span class="line">            <span class="keyword">if</span> ( <span class="built_in">abs</span>(nums[i]) &gt; <span class="built_in">abs</span>(nums[j]) ) &#123;</span><br><span class="line">                res[t]=nums[i]*nums[i];</span><br><span class="line">                <span class="comment">//res.insert(res.begin(),nums[i]*nums[i]);</span></span><br><span class="line">                i++;t--;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                res[t]=nums[j]*nums[j];</span><br><span class="line">                <span class="comment">//res.insert(res.begin(),nums[j]*nums[j]);</span></span><br><span class="line">                j--;t--;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h1 id="Leetcode-209-长度最小的子数组-24-12-17"><a href="#Leetcode-209-长度最小的子数组-24-12-17" class="headerlink" title="Leetcode 209. 长度最小的子数组 24.12.17"></a>Leetcode <a href="https://leetcode.cn/problems/minimum-size-subarray-sum/">209. 长度最小的子数组</a> 24.12.17</h1><h2 id="题面-3"><a href="#题面-3" class="headerlink" title="题面"></a>题面</h2><p>给定一个含有 <code>n</code> 个正整数的数组和一个正整数 <code>target</code> <strong>。</strong></p>
<p>找出该数组中满足其总和大于等于 <code>target</code> 的长度最小的 </p>
<p><strong>子数组</strong></p>
<p><code>[numsl, numsl+1, ..., numsr-1, numsr]</code> ，并返回其长度<strong>。</strong>如果不存在符合条件的子数组，返回 <code>0</code> 。</p>
<p><strong>示例 1：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：target = 7, nums = [2,3,1,2,4,3]</span><br><span class="line">输出：2</span><br><span class="line">解释：子数组 [4,3] 是该条件下的长度最小的子数组。</span><br></pre></td></tr></table></figure>
<p><strong>示例 2：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：target = 4, nums = [1,4,4]</span><br><span class="line">输出：1</span><br></pre></td></tr></table></figure>
<p><strong>示例 3：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：target = 11, nums = [1,1,1,1,1,1,1,1]</span><br><span class="line">输出：0</span><br></pre></td></tr></table></figure>
<p><strong>提示：</strong></p>
<ul>
<li><code>1 &lt;= target &lt;= 109</code></li>
<li><code>1 &lt;= nums.length &lt;= 105</code></li>
<li><code>1 &lt;= nums[i] &lt;= 104</code></li>
</ul>
<p><strong>进阶：</strong></p>
<ul>
<li>如果你已经实现 <code>O(n)</code> 时间复杂度的解法, 请尝试设计一个 <code>O(n log(n))</code> 时间复杂度的解法。</li>
</ul>
<h2 id="题解：滑动窗口"><a href="#题解：滑动窗口" class="headerlink" title="题解：滑动窗口"></a>题解：滑动窗口</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">minSubArrayLen</span><span class="params">(<span class="type">int</span> target, vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> i=<span class="number">0</span>;<span class="comment">//起始</span></span><br><span class="line">        <span class="type">int</span> sum=<span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> res=<span class="number">99999999</span>;</span><br><span class="line">        <span class="type">int</span> len=<span class="number">0</span>;</span><br><span class="line">        <span class="comment">//j为终止下标</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j=<span class="number">0</span>;j&lt;nums.<span class="built_in">size</span>();j++)&#123;</span><br><span class="line">            sum+=nums[j];</span><br><span class="line">            <span class="keyword">while</span>(sum&gt;=target)&#123;</span><br><span class="line">                len=j-i<span class="number">+1</span>;</span><br><span class="line">                res=<span class="built_in">min</span>(res,len);</span><br><span class="line">                sum-=nums[i];</span><br><span class="line">                i++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (sum&lt;target&amp;&amp;res==<span class="number">99999999</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h1 id="Leetcode-59-螺旋矩阵-II-24-12-17"><a href="#Leetcode-59-螺旋矩阵-II-24-12-17" class="headerlink" title="Leetcode 59. 螺旋矩阵 II  24.12.17"></a>Leetcode <a href="https://leetcode.cn/problems/spiral-matrix-ii/">59. 螺旋矩阵 II </a> 24.12.17</h1><p>给你一个正整数 <code>n</code> ，生成一个包含 <code>1</code> 到 <code>n2</code> 所有元素，且元素按顺时针顺序螺旋排列的 <code>n x n</code> 正方形矩阵 <code>matrix</code> 。</p>
<p><strong>示例 1：</strong></p>
<p><img src="https://assets.leetcode.com/uploads/2020/11/13/spiraln.jpg" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：n = 3</span><br><span class="line">输出：[[1,2,3],[8,9,4],[7,6,5]]</span><br></pre></td></tr></table></figure>
<p><strong>示例 2：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：n = 1</span><br><span class="line">输出：[[1]]</span><br></pre></td></tr></table></figure>
<p><strong>提示：</strong></p>
<ul>
<li><code>1 &lt;= n &lt;= 20</code></li>
</ul>
<h2 id="题解：模拟，注意创建二维数组方式"><a href="#题解：模拟，注意创建二维数组方式" class="headerlink" title="题解：模拟，注意创建二维数组方式"></a>题解：模拟，注意创建二维数组方式</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">generateMatrix</span>(<span class="type">int</span> n) &#123;</span><br><span class="line">        <span class="comment">//采用左闭右开</span></span><br><span class="line">        vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">matrix</span>(n,<span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(n));</span><br><span class="line">        <span class="type">int</span> startx=<span class="number">0</span>;<span class="type">int</span> starty=<span class="number">0</span>;<span class="comment">//起始位置</span></span><br><span class="line">        <span class="type">int</span> offset=<span class="number">1</span>;<span class="comment">//右开</span></span><br><span class="line">        <span class="type">int</span> t=n/<span class="number">2</span>;<span class="comment">//转几圈</span></span><br><span class="line">        <span class="type">int</span> i,j;</span><br><span class="line">        <span class="type">int</span> count=<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span>(t)&#123;</span><br><span class="line">            <span class="comment">//从左到右</span></span><br><span class="line">            <span class="keyword">for</span> (j=starty;j&lt;n-offset;j++)&#123;</span><br><span class="line">                matrix[startx][j]=count++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//从上到下</span></span><br><span class="line">            <span class="keyword">for</span> (i=startx;i&lt;n-offset;i++)&#123;</span><br><span class="line">                matrix[i][j]=count++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//从右到左</span></span><br><span class="line">            <span class="keyword">for</span> (;j&gt;starty;j--)&#123;</span><br><span class="line">                matrix[i][j]=count++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//从下到上</span></span><br><span class="line">            <span class="keyword">for</span> (;i&gt;startx;i--)&#123;</span><br><span class="line">                matrix[i][j]=count++;</span><br><span class="line">            &#125;</span><br><span class="line">            startx++;starty++;offset++;</span><br><span class="line">            t--;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (n%<span class="number">2</span>==<span class="number">1</span>)&#123;</span><br><span class="line">            matrix[n/<span class="number">2</span>][n/<span class="number">2</span>]=count++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> matrix;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h1 id="Leetcode-203-移除链表元素"><a href="#Leetcode-203-移除链表元素" class="headerlink" title="Leetcode 203. 移除链表元素"></a>Leetcode <a href="https://leetcode.cn/problems/remove-linked-list-elements/">203. 移除链表元素</a></h1><h2 id="题面-4"><a href="#题面-4" class="headerlink" title="题面"></a>题面</h2><p>给你一个链表的头节点 <code>head</code> 和一个整数 <code>val</code> ，请你删除链表中所有满足 <code>Node.val == val</code> 的节点，并返回 <strong>新的头节点</strong> 。</p>
<p><strong>示例 1：</strong></p>
<p><img src="https://assets.leetcode.com/uploads/2021/03/06/removelinked-list.jpg" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：head = [1,2,6,3,4,5,6], val = 6</span><br><span class="line">输出：[1,2,3,4,5]</span><br></pre></td></tr></table></figure>
<p><strong>示例 2：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：head = [], val = 1</span><br><span class="line">输出：[]</span><br></pre></td></tr></table></figure>
<p><strong>示例 3：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：head = [7,7,7,7], val = 7</span><br><span class="line">输出：[]</span><br></pre></td></tr></table></figure>
<p><strong>提示：</strong></p>
<ul>
<li>列表中的节点数目在范围 <code>[0, 104]</code> 内</li>
<li><code>1 &lt;= Node.val &lt;= 50</code></li>
<li><code>0 &lt;= val &lt;= 50</code></li>
</ul>
<h2 id="题解：虚拟头节点"><a href="#题解：虚拟头节点" class="headerlink" title="题解：虚拟头节点"></a>题解：虚拟头节点</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"> * struct ListNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     ListNode *next;</span></span><br><span class="line"><span class="comment"> *     ListNode() : val(0), next(nullptr) &#123;&#125;</span></span><br><span class="line"><span class="comment"> *     ListNode(int x) : val(x), next(nullptr) &#123;&#125;</span></span><br><span class="line"><span class="comment"> *     ListNode(int x, ListNode *next) : val(x), next(next) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//方法一：虚拟头节点，这种方法比较正常</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">removeElements</span><span class="params">(ListNode* head, <span class="type">int</span> val)</span> </span>&#123;</span><br><span class="line">        ListNode* dhead= <span class="keyword">new</span> ListNode;</span><br><span class="line">        dhead-&gt;next=head;</span><br><span class="line">        ListNode* cur=dhead;</span><br><span class="line">        <span class="keyword">while</span>(cur-&gt;next!=<span class="literal">NULL</span>)&#123;</span><br><span class="line">            <span class="keyword">if</span> (cur-&gt;next-&gt;val==val)&#123;</span><br><span class="line">                cur-&gt;next=cur-&gt;next-&gt;next;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                cur=cur-&gt;next;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dhead-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//方法二：不创建虚拟头节点，直接使用第一个元素作为head，需要多处理一下删除头节点的情况，相对复杂</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">removeElements</span><span class="params">(ListNode* head, <span class="type">int</span> val)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (head!=<span class="literal">NULL</span>&amp;&amp;head-&gt;val==val)&#123;</span><br><span class="line">            head=head-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        ListNode* cur=head;</span><br><span class="line">        <span class="keyword">if</span> (cur==<span class="literal">NULL</span>) <span class="keyword">return</span> head;</span><br><span class="line">        <span class="keyword">while</span>(cur-&gt;next!=<span class="literal">NULL</span>&amp;&amp;cur!=<span class="literal">NULL</span>)&#123;</span><br><span class="line">            <span class="keyword">if</span> (cur-&gt;next-&gt;val==val)&#123;</span><br><span class="line">                cur-&gt;next=cur-&gt;next-&gt;next;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                cur=cur-&gt;next;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> head;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h1 id="Leetcode-707-设计链表-1-12"><a href="#Leetcode-707-设计链表-1-12" class="headerlink" title="Leetcode 707. 设计链表 1.12"></a>Leetcode <a href="https://leetcode.cn/problems/design-linked-list/">707. 设计链表 </a>1.12</h1><h1 id="题面-5"><a href="#题面-5" class="headerlink" title="题面"></a>题面</h1><p>你可以选择使用单链表或者双链表，设计并实现自己的链表。</p>
<p>单链表中的节点应该具备两个属性：<code>val</code> 和 <code>next</code> 。<code>val</code> 是当前节点的值，<code>next</code> 是指向下一个节点的指针/引用。</p>
<p>如果是双向链表，则还需要属性 <code>prev</code> 以指示链表中的上一个节点。假设链表中的所有节点下标从 <strong>0</strong> 开始。</p>
<p>实现 <code>MyLinkedList</code> 类：</p>
<ul>
<li><code>MyLinkedList()</code> 初始化 <code>MyLinkedList</code> 对象。</li>
<li><code>int get(int index)</code> 获取链表中下标为 <code>index</code> 的节点的值。如果下标无效，则返回 <code>-1</code> 。</li>
<li><code>void addAtHead(int val)</code> 将一个值为 <code>val</code> 的节点插入到链表中第一个元素之前。在插入完成后，新节点会成为链表的第一个节点。</li>
<li><code>void addAtTail(int val)</code> 将一个值为 <code>val</code> 的节点追加到链表中作为链表的最后一个元素。</li>
<li><code>void addAtIndex(int index, int val)</code> 将一个值为 <code>val</code> 的节点插入到链表中下标为 <code>index</code> 的节点之前。如果 <code>index</code> 等于链表的长度，那么该节点会被追加到链表的末尾。如果 <code>index</code> 比长度更大，该节点将 <strong>不会插入</strong> 到链表中。</li>
<li><code>void deleteAtIndex(int index)</code> 如果下标有效，则删除链表中下标为 <code>index</code> 的节点。</li>
</ul>
<p><strong>示例：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入</span><br><span class="line">[&quot;MyLinkedList&quot;, &quot;addAtHead&quot;, &quot;addAtTail&quot;, &quot;addAtIndex&quot;, &quot;get&quot;, &quot;deleteAtIndex&quot;, &quot;get&quot;]</span><br><span class="line">[[], [1], [3], [1, 2], [1], [1], [1]]</span><br><span class="line">输出</span><br><span class="line">[null, null, null, null, 2, null, 3]</span><br><span class="line"></span><br><span class="line">解释</span><br><span class="line">MyLinkedList myLinkedList = new MyLinkedList();</span><br><span class="line">myLinkedList.addAtHead(1);</span><br><span class="line">myLinkedList.addAtTail(3);</span><br><span class="line">myLinkedList.addAtIndex(1, 2);    // 链表变为 1-&gt;2-&gt;3</span><br><span class="line">myLinkedList.get(1);              // 返回 2</span><br><span class="line">myLinkedList.deleteAtIndex(1);    // 现在，链表变为 1-&gt;3</span><br><span class="line">myLinkedList.get(1);              // 返回 3</span><br></pre></td></tr></table></figure>
<p><strong>提示：</strong></p>
<ul>
<li><code>0 &lt;= index, val &lt;= 1000</code></li>
<li>请不要使用内置的 LinkedList 库。</li>
<li>调用 <code>get</code>、<code>addAtHead</code>、<code>addAtTail</code>、<code>addAtIndex</code> 和 <code>deleteAtIndex</code> 的次数不超过 <code>2000</code> 。</li>
</ul>
<h2 id="题解：挺红的，这给一个样例改一次bug，改了有半小时还不止，关注addAtTail，addAtIndex，deleteAtIndex函数，它们都从head开始遍历，不从head-gt-next开始"><a href="#题解：挺红的，这给一个样例改一次bug，改了有半小时还不止，关注addAtTail，addAtIndex，deleteAtIndex函数，它们都从head开始遍历，不从head-gt-next开始" class="headerlink" title="题解：挺红的，这给一个样例改一次bug，改了有半小时还不止，关注addAtTail，addAtIndex，deleteAtIndex函数，它们都从head开始遍历，不从head-&gt;next开始"></a>题解：挺红的，这给一个样例改一次bug，改了有半小时还不止，关注addAtTail，addAtIndex，deleteAtIndex函数，它们都从head开始遍历，不从head-&gt;next开始</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 题目当中的ListNode的结构是这样的</span></span><br><span class="line"><span class="comment">// struct ListNode&#123;</span></span><br><span class="line"><span class="comment">//     int val;</span></span><br><span class="line"><span class="comment">//     ListNode *next;</span></span><br><span class="line"><span class="comment">// &#125;;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinkedList</span> &#123;</span><br><span class="line"><span class="comment">//public:</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">int</span> size;</span><br><span class="line">    ListNode *head;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">MyLinkedList</span>() &#123;</span><br><span class="line">        size=<span class="number">0</span>;</span><br><span class="line">        head = <span class="keyword">new</span> ListNode;</span><br><span class="line">        head-&gt;val=<span class="number">0</span>;</span><br><span class="line">        head-&gt;next=<span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 第一个元素的下标是0，从虚拟头节点的下一个开始向后遍历</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">get</span><span class="params">(<span class="type">int</span> index)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (index&lt;<span class="number">0</span>||index&gt;size<span class="number">-1</span>) <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        ListNode *cur = head-&gt;next;</span><br><span class="line">        <span class="keyword">while</span> (index--)&#123;</span><br><span class="line">            cur=cur-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> cur-&gt;val;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">addAtHead</span><span class="params">(<span class="type">int</span> val)</span> </span>&#123;</span><br><span class="line">        ListNode *t=<span class="keyword">new</span> ListNode;</span><br><span class="line">        t-&gt;val=val;</span><br><span class="line">        t-&gt;next=head-&gt;next;</span><br><span class="line">        head-&gt;next=t;    </span><br><span class="line">        size++;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">addAtTail</span><span class="params">(<span class="type">int</span> val)</span> </span>&#123;</span><br><span class="line">        ListNode *t = <span class="keyword">new</span> ListNode;</span><br><span class="line">        ListNode *cur = head;</span><br><span class="line">        <span class="keyword">while</span> (cur-&gt;next!=<span class="literal">NULL</span>)&#123;</span><br><span class="line">            cur= cur-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        cur-&gt;next=t;</span><br><span class="line">        t-&gt;next=<span class="literal">NULL</span>;</span><br><span class="line">        t-&gt;val=val;</span><br><span class="line">        size++;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">addAtIndex</span><span class="params">(<span class="type">int</span> index, <span class="type">int</span> val)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (index&gt;size) <span class="keyword">return</span> ;</span><br><span class="line">        ListNode *t = <span class="keyword">new</span> ListNode;</span><br><span class="line">        ListNode *cur = head;</span><br><span class="line">        <span class="comment">//index-=1;</span></span><br><span class="line">        <span class="keyword">while</span> (index&gt;<span class="number">0</span>)&#123;</span><br><span class="line">            cur=cur-&gt;next;</span><br><span class="line">            index--;</span><br><span class="line">        &#125;</span><br><span class="line">        t-&gt;next=cur-&gt;next;</span><br><span class="line">        cur-&gt;next=t;</span><br><span class="line">        t-&gt;val=val;</span><br><span class="line">        size++;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">deleteAtIndex</span><span class="params">(<span class="type">int</span> index)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (index&lt;<span class="number">0</span>||index&gt;size<span class="number">-1</span>) <span class="keyword">return</span> ;</span><br><span class="line">        ListNode *cur=head;</span><br><span class="line">        <span class="comment">//index-=1;</span></span><br><span class="line">        <span class="keyword">while</span> (index&gt;<span class="number">0</span>)&#123;</span><br><span class="line">            cur=cur-&gt;next;</span><br><span class="line">            index--;</span><br><span class="line">        &#125;</span><br><span class="line">        cur-&gt;next=cur-&gt;next-&gt;next;</span><br><span class="line">        size--;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Your MyLinkedList object will be instantiated and called as such:</span></span><br><span class="line"><span class="comment"> * MyLinkedList* obj = new MyLinkedList();</span></span><br><span class="line"><span class="comment"> * int param_1 = obj-&gt;get(index);</span></span><br><span class="line"><span class="comment"> * obj-&gt;addAtHead(val);</span></span><br><span class="line"><span class="comment"> * obj-&gt;addAtTail(val);</span></span><br><span class="line"><span class="comment"> * obj-&gt;addAtIndex(index,val);</span></span><br><span class="line"><span class="comment"> * obj-&gt;deleteAtIndex(index);</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>算法学习</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>GPS</title>
    <url>/2025/01/12/GPS/</url>
    <content><![CDATA[<hr>
<p><a href="https://www.semanticscholar.org/paper/Greedy-Policy-Search%3A-A-Simple-Baseline-for-Molchanov-Lyzhov/8286fbad6d4712d950689930720134bc8828963f?sort=relevance&amp;queryString=segmentation"> Greedy Policy Search: A Simple Baseline for Learnable Test-Time Augmentation | Semantic Scholar</a></p>
<p><a href="https://github.com/SamsungLabs/gps-augment?tab=readme-ov-file">SamsungLabs/gps-augment: Simple but high-performing method for learning a policy of test-time augmentation (github.com)</a></p>
<span id="more"></span>
<p>在一个已经训练好的网络上将测试数据增广，利用这些数据来测量网络的不确定性</p>
<p>GPS（greedy policy search）：a simple algorithm that learns a policy for test-time data augmentation based on the predictive performance on a validation set. </p>
<p>一个简单的算法，<strong>根据验证集的预测性能</strong>来学习测试时间数据增强的策略。</p>
<p>In an ablation study, we show that <strong>optimizing the calibrated log-likelihood**</strong>（优化校准的对数似然）<strong> (Ashukha et al., 2020) is </strong>a crucial part<strong> of the policy search algorithm, while the default objectives—</strong>accuracy and loglikelihood—lead to a significant drop in the final performance.**</p>
<p>定义TTA policy为一系列子政策的集合，每个子政策又包含Ns个连续的图片变换。</p>
<p>步骤：</p>
<p>1、 先用CIFAR10数据集训练VGG模型，并用randaugment增广数据</p>
<p>2、 将这些增广好的数据进行预测处理并选取其中的一部分作为备选池。本文共选取1101个子政策</p>
<p>3、 用校准的对数似然来评价，选择让可以在聚合预测时提供最大提升的子政策进入pool中</p>
<p><strong>Prediction通过求不同子政策的平均得到</strong></p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image002-1736691190942-1.jpg)</p>
<p>GPS的核心算法（利用贪心和校准的对数似然）</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image004-1736691190944-2.png)</p>
<p>评判标准：校准的对数似然（对数似然加上temperature scaling）</p>
<p>GPS是一个学习型（learnable）的策略，它在过程中不断选择好的东西放到pool中</p>
<p>域内的表现不错，但是还可以加上聚合来加强表现。</p>
<p>原本的指标（metric）是accuracy和log-likelihood。但是LL可能会误把好模型忽略（因为刚好无法校准）但可以用温度缩放来调整回来。cLL可以得到更好的M</p>
<p>领域转移的健壮性：CIFAR10-C, CIFAR-100-C and ImageNet-C datasets with 15 corruptions C from groups noise, blur, weather and digital.（数据）对每个确定的level都计算错误率，对每个corruption都计算平均</p>
<p>一般来看，在干净的验证数据上训练的策略对损坏的数据有很好的效果。（换句话说，在哪里GPS效果都很好） transfer GPS policy 效果在不同的数据集上不错</p>
<p>结合聚合方法效果也不错</p>
<p>在原始的数据增广上也有不错的表现</p>
<p>未加valid 5000</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image006-1736691190944-3.png)</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image008-1736691190944-4.png)</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image010-1736691190944-5.png)</p>
<p>校准之后的数据</p>
<p>加了valid5000</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image012.jpg)</p>
<p>训练randaugment</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image014-1736691190944-6.png)</p>
<p>第一次增广预测</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image016-1736691190944-8.png)</p>
<p>第二次增广预测</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image018.png)</p>
<p>第三次增广预测</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image020-1736691190944-7.png)</p>
<p>第四次增广预测</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image022.png)</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image024-1736691190945-9.png)</p>
<p>GPS过程</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image026-1736691190945-10.png)评价过程</p>
<p>Reference：</p>
<p><a href="https://zhuanlan.zhihu.com/p/620688513">Pytorch格式 .pt .pth .bin .onnx 详解 - 知乎 (zhihu.com)</a></p>
<p><a href="https://blog.csdn.net/m0_51004308/article/details/118001835">Pytorch学习笔记(七):F.softmax()和F.log_softmax函数详解-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/qiankendeNMY/article/details/126508781?spm=1018.2226.3001.9630.1&amp;extra[title]=Python—argparse模块&amp;extra[utm_source]=vip_chatgpt_common_pc_toolbar&amp;extra[utm_medium]=">Python—argparse模块_parser.add_argument(‘—data’, type=str, default=’’-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/tsinghuahui/article/details/89279152">python argparse中action的可选参数store_true的作用_argparse store_true-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/u010804417/article/details/92843528">伪代码书写规则_伪代码怎么写-CSDN博客</a>（在伪码中，符号∅通常表示空集或空值）</p>
<p><a href="https://zhuanlan.zhihu.com/p/26614750">一文搞懂极大似然估计 - 知乎 (zhihu.com)</a></p>
<p><a href="https://blog.csdn.net/Elenstone/article/details/105677886">机器学习最易懂之贝叶斯模型详解与python实现_pytorch实现贝叶斯决策模型-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/qq_23933415/article/details/111212898">模型的参数verbose用法详解_verbose参数-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/weixin_44912159/article/details/104921422">论文中的常见缩写(w.r.t/i.e./et al等)的意思-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/fengbingchun/article/details/124648766">深度学习中的优化算法之带Momentum的SGD_momentum sgd-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/wangxuecheng666/article/details/109246126">StratifiedShuffleSplit（）函数的详细理解-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/qq_37297763/article/details/116847455">Pytorch学习（十三）python中<em>args和*</em>kwargs的用法_pytorch args.-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/Just_do_myself/article/details/123751148?spm=1018.2226.3001.9630.1&amp;extra[title]=深度学习之动态调整学习率LR&amp;extra[utm_source]=vip_chatgpt_common_pc_toolbar&amp;extra[utm_medium]=">深度学习之动态调整学习率LR_动态学习率-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/qq_42079689/article/details/102873766">PyTorch的nn.Linear（）详解_nn.linear()-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/weixin_42843425/article/details/122518502">model.parameters()的理解与使用-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/qq_41813454/article/details/135129279?spm=1018.2226.3001.9630.1&amp;extra[title]=别再混淆了！model.eval(">别再混淆了！model.eval()和torch.no_grad()的区别一次讲清楚-CSDN博客</a>和torch.no_grad()的区别一次讲清楚&amp;extra[utm_source]=vip_chatgpt_common_pc_toolbar&amp;extra[utm_medium]=)</p>
<p><a href="https://zhuanlan.zhihu.com/p/38121870">机器学习之K折交叉验证 - 知乎 (zhihu.com)</a></p>
<p><a href="https://blog.csdn.net/qq_39478403/article/details/121107057">【机器学习】浅谈 归纳偏置 (Inductive Bias)-CSDN博客</a></p>
<p>· store_true 是指带触发 action 时为真，不触发则为假， 即默认 False ，传参 则 设置为 True<br> · store_false 则与之相反</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image028-1736691190945-11.png)<img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image030-1736691190945-12.png)<img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image032-1736691190945-13.png)</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image034-1736691190945-14.png)<img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image036-1736691190945-16.png)</p>
<p>enumerate函数，它可以同时返回列表中的元素和它们的索引</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image038-1736691190945-15.png)</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image040-1736691190945-18.png)</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image042.png)</p>
<p><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image044.png)</p>
<p>ablation study (消融实验）</p>
<p><a href="https://zhuanlan.zhihu.com/p/644502891">一文搞懂什么是ablation study (消融实验） - 知乎 (zhihu.com)</a><br> adversarial attacks 对抗攻击 在某些微小影响下会导致巨大偏差</p>
<p><a href="https://zhuanlan.zhihu.com/p/104532285">CV||对抗攻击领域综述（adversarial attack） - 知乎 (zhihu.com)</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/293065153">Google开箱即用数据增强RandAugment | NeurIPS 2020 - 知乎 (zhihu.com)</a></p>
<p>原始的自动数据增强方式，先在一个单独的小数据集代理任务（可以理解成小/子数据集，子任务 ）上进行，然后再迁移到更大数据的目标任务中去。作者提出了一种实用的自动数据增强算法RandAugment。该方法无需在子任务上进行搜索验证。但为了减少数据增强的参数搜索空间，作者设计了一种简单的格点搜索（simple grid search）方法以学习/获取一种数据增强的策略。<strong>RandAugment只有两个参数：N和M。 其中N为在每次增强时使用N次操作（使用的这N个操作，都是从操作集中等概率抽取的，例如操作集中有14种操作，则每种操作被选中的概率为1/14，每张图像的N次增强中，选到的操作可能是一样的），M为正整数，表示所有操作在应用时，幅度都为M（旋转多少）</strong>。</p>
<p><strong>用于数据增广</strong><img src="D:\Program Files (x86" alt="img">\Typora\img-data\clip_image046-1736691190945-17.png)</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>校准</tag>
      </tags>
  </entry>
</search>
