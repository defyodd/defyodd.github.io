<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>U-Net</title>
    <url>/undefined</url>
    <content><![CDATA[<p>U-Net</p>
<p>​                               </p>
<p>The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization.</p>
<p>该架构由一个捕捉上下文的收缩路径和一个实现精确定位的对称扩展路径组成</p>
<p><strong>Hence, Ciresan et al.trained a network in a sliding-window setup to predict the class label of each pixel by providing a local region (patch) around that pixel as input.</strong></p>
<p><strong>This network can localize.</strong></p>
<p><strong>Two drawbacks: slow and redundant ,trade-off between localization accuracy and the use of context.</strong></p>
<p>The main idea in fully convolutional network is to supplement a usual contracting network by successive layers, where pooling operators are replaced by upsampling operators.</p>
<p>全卷积网络的主要思想是通过<strong>连续的层来补充通常的收缩网络，其中池化算子被上采样算子所取代。</strong></p>
<p>FCN的主要工作是将传统CNN后面的全连接层换成了卷积层，这样网络的输出将是热力图而非类别；同时，为解决卷积和池化导致图像尺寸的变小，使用上采样方式对图像尺寸进行恢复，并且提出一个跳级结构，让pool4和pool3之后的特征图于conv7之后的特征图相加，得到新的更加准确的分割图像。</p>
<p>这些layers增加了输出的分辨率。为了定位，通过收缩路径(conracting path)的高分辨率特征图和上采样的输出相结合，之后的连续卷积层可以来通过这些信息来学习更加精准的输出。</p>
<p><strong>对于上采样保留了很多的特征通道，这个网络没有全连接层，只有卷积层。</strong></p>
<p>只包含卷积就保证了seamless segmentation of arbitrarily large images by an overlap-tile strategy。通过重叠瓦片技术对任意大图形进行无缝分割。通过mirroring the input image.</p>
<p>To predict the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image. This tiling strategy is important to apply the network to large images, since otherwise the resolution would be limited by the GPU memory.</p>
<p><strong>针对于数据增广，文章使用了弹性变换的方式，使得网络能够学习对这种变形的不变性，而不需要在注释的图像语料库中看到这些转换。</strong></p>
<p>对同类型的细胞的分离是很打的一个挑战。文章使用了weighted loss，针对不同地区进行不同的loss学习，让网络对边缘(border)进行学习，达到分离的效果。</p>
<p>网络结构：</p>
<p>左边：contracting path(收缩路径) 右边：expansive path(扩展路径)</p>
<p>Contracting path :重复使用2个没有padding的3<em>3卷积，每个卷积之后包含ReLU和一个2</em>2最大池化（stride&#x3D;2），同时，在每次下采样的过程中将通道数*2.</p>
<p>Expansive path : 2<em>2卷积进行上采样同时将通道数&#x2F;2， 与收缩路径中相应裁剪的特征图相连接，两个3</em>3卷积每个都跟着一个ReLU。Cropping是重要的因为在卷积操作中会造成边缘像素的丢失。</p>
<p>最后一层有一个1*1的卷积将64个特征向量映射到每个所期待的类别。</p>
<p>总共有23层。</p>
<p>为了允许输出分割图的无缝平铺（见图2），重要的是选择输入平铺的大小，使所有2x2的最大池化操作都应用于一个具有偶数x和y大小的层。（换句话说，contracting path中经过卷积和ReLU之后的图像的x，y都是偶数）</p>
<p>训练：</p>
<p>To minimize the overhead and make maximum use of the GPU memory, we favor large input tiles over a large batch size and hence reduce the batch to a single image.</p>
<p>为了最大限度地减少开销和最大限度地利用GPU内存，我们倾向于使用大的输入瓦片而不是大的批处理量，从而将批处理量减少到单一图像。（换句话说就是图像大，batch size小）</p>
<p>Accordingly we use a high momentum (0.99) such that a large number of the previously seen training samples determine the update in the current optimization step.</p>
<p>因此，我们使用一个高动量（0.99），这样大量的先前看到的训练样本决定了当前优化步骤中的更新。</p>
<p>能量函数(energy function)一开始在热力学中被定义，用于描述系统的能量值，当能量值达到最小时系统达到稳定状态。（换句话说就是优化函数）</p>
<p>energy function： pixel-wise soft-max + Cross entropy</p>
<p>Weighted map：</p>
<p>需要有一个好的初始化a good initialization of the weights is extremely important</p>
<p>Ideally the initial weights should be adapted such that each feature map in the network has approximately unit variance. 理想情况下，初始权重应该被调整，使网络中的每个特征图具有近似的单位方差。</p>
<p>可以通过一个标准差为  的高斯分布得到。</p>
<p>Data augmentation</p>
<p>random elastic deformations is key concept..</p>
<p>We generate smooth deformations using random displacement vectors on a coarse 3 by 3 grid. The displacements are sampled from a Gaussian distribution with 10 pixels standard deviation. Per-pixel displacements are then computed using bicubic interpolation. Drop-out layers at the end of the contracting path perform further implicit data augmentation</p>
<p>我们在一个粗略的3乘3的网格上使用随机位移矢量生成平滑的变形。位移是从标准偏差为10像素的高斯分布中采样的。然后使用双三次插值计算每像素的位移。在收缩路径末端的剔除层进行进一步的隐性数据增强</p>
<p>Reference:</p>
<p><a href="https://blog.csdn.net/qq_36717487/article/details/115368483">语义分割与实例分割的区别_实例分割和语义分割-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/lsb2002/article/details/132005658">机器学习：监督学习、无监督学习、半监督学习、强化学习-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/qq_41731861/article/details/120511148">FCN（全卷积神经网络）详解-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/maliang_1993/article/details/82020596">仿射和弹性变换（affine and elastic transform）的python实现_图像增强弹性变换-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/u011622208/article/details/112566676">【数据增强】——弹性变形(Elastic Distortion)_数据增强随机弹性变换-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/weixin_36474809/article/details/87931260">Unet论文详解U-Net:Convolutional Networks for Biomedical Image Segmentation-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/gaoxueyi551/article/details/105238182">深度学习中的Momentum算法原理-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/qq_43374104/article/details/106434440">梯度下降优化算法Momentum_momentum梯度下降法-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/hejunqing14/article/details/50057001">能量函数在神经网络中的意义_energy function-CSDN博客</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>论文学习</tag>
      </tags>
  </entry>
  <entry>
    <title>u-kan</title>
    <url>/undefined</url>
    <content><![CDATA[<p><a href="https://blog.csdn.net/jarodyv/article/details/138751803">深入理解 Kolmogorov–Arnold Networks (KAN)_kan: kolmogorov–arnold networks-CSDN博客</a></p>
<p><a href="https://www.zhihu.com/question/654782350">(5 封私信 &#x2F; 14 条消息) 如何评价神经网络架构KAN，是否有潜力取代MLP？ - 知乎 (zhihu.com)</a></p>
<p><a href="https://blog.csdn.net/fg13821267836/article/details/93405572">多层感知机（MLP）简介-CSDN博客</a></p>
<p>MLP说白了就是最简单的$$f(w_i×x+b_i)+softmax$$.</p>
<p><a href="https://blog.csdn.net/qq_46703208/article/details/130539464">深度学习中的token_深度学习token-CSDN博客</a></p>
<p>token:最小的单元。在nlp领域，就是一个单词。cv领域，在vit中就是16x16的patch</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>论文学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Temperature scaling</title>
    <url>/undefined</url>
    <content><![CDATA[<p>Temperature scaling</p>
<p>后验概率被应用在训练过程之后目的是学习一个重新校准的函数。为了达到这个目的，训练集的一部分被拿出作为校准集。重新校准函数适用于网络的输出（如logit向量），并产生一个改进的校准，该校准是在遗漏的校准集上学习的。</p>
<p>Temperature scaling是data-efficiency的accuracy-preserving方法，但是具有稍微不太好的expressive</p>
<p>Selective Scaling：</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image002-1736691287914-3.png)</p>
<p>原始：</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image004-1736691287914-4.png)![img](D:\Program Files (x86)\Typora\img-data\clip_image006-1736691287914-2.png)</p>
<p>只有卷积：![img](D:\Program Files (x86)\Typora\img-data\clip_image007.png)</p>
<p>只有池化：</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image009.png)</p>
<p>只有池化：</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image011.png)![img](D:\Program Files (x86)\Typora\img-data\clip_image013.png)</p>
<p>1.5</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image015.png)</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image017-1736691287913-1.png)</p>
<p>1.6：</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image019-1736691287914-5.png)![img](D:\Program Files (x86)\Typora\img-data\clip_image021.png)</p>
<p>2.0：</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image023.png)![img](D:\Program Files (x86)\Typora\img-data\clip_image025.png)</p>
<p>1.8+ece做loss</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image027.png)![img](D:\Program Files (x86)\Typora\img-data\clip_image029.png)</p>
<p>1.8+ece做loss+池化</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image031-1736691287914-6.png)![img](D:\Program Files (x86)\Typora\img-data\clip_image033.png)</p>
<p>Reference：</p>
<p><a href="https://blog.csdn.net/m0_51233386/article/details/128414228">Ubuntu下tar命令使用详解 .tar解压、.tar压缩_ubuntu解压tar.gz-CSDN博客</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/630739668">Logistic（逻辑斯蒂）函数浅谈 - 知乎 (zhihu.com)</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/335765830">简介欧拉第一类积分：Beta函数 - 知乎 (zhihu.com)</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/63184325">神经网络1：多层感知器-MLP - 知乎 (zhihu.com)</a></p>
<p><a href="https://blog.csdn.net/lingzhou33/article/details/87901365">语义分割的评价指标——IoU_语义分割iou-CSDN博客</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/33841176">CNN 入门讲解：什么是全连接层（Fully Connected Layer）? - 知乎 (zhihu.com)</a></p>
<p><a href="https://blog.csdn.net/qq_37369201/article/details/109195257">python中assert的用法（简洁明了）_assert python-CSDN博客</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/666048268">Expected Calibration Error (ECE) - 知乎 (zhihu.com)</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>校准</tag>
      </tags>
  </entry>
  <entry>
    <title>PVT encoder</title>
    <url>/undefined</url>
    <content><![CDATA[<h1 id="PVT-encoder"><a href="#PVT-encoder" class="headerlink" title="PVT encoder"></a>PVT encoder</h1><p>use of transformer</p>
<p>For example, some works model the vision task as a dictionary lookup problem with learnable queries, and use the Transformer decoder as a task-specific head on top of the CNN backbone.</p>
<p>将视觉任务建模为一个具有可学习查询的字典查询问题，并将Transformer解码器作为CNN主干之上的特定任务头。</p>
<h2 id="ViT"><a href="#ViT" class="headerlink" title="ViT"></a>ViT</h2><p>拥有圆柱形的粗略images patches as input </p>
<p>shortcomings：</p>
<ol>
<li>输出特征图是单尺度而且底分辨率</li>
<li>计算花销大</li>
</ol>
<h2 id="PVT"><a href="#PVT" class="headerlink" title="PVT"></a>PVT</h2><p>a pure transformer backbone </p>
<p>advantages:</p>
<ol>
<li>细粒度(fine-grained)的图像patches 来学习高分辨率表征</li>
<li>引入一个逐渐减小的金字塔transformer的序列长度，减少计算量</li>
<li>引入空间降低注意力层(spatial-reduce attention layer)降低在学习高分辨率特征图的开销</li>
<li>产生全局感受野(global reception field)</li>
</ol>
<p>![image-20240812121219476](D:\Program Files (x86)\Typora\img-data\image-20240812121219476.png)</p>
<p>Feature Pyramid for Transformer</p>
<p>首先将输入特征图分为$$\frac{H_i W_i}{P_i^2}$$个patches，将每个patches展平之后并且将其映射到(projected to)$$C_i$$维度的embedding，再经过一个线性映射之后，embedded patches可以被视为$$\frac{H_i-1}{P_i}\times \frac{W_i-1}{P_i} \times C_i$$</p>
<h2 id="SRA-？"><a href="#SRA-？" class="headerlink" title="SRA ？"></a>SRA ？</h2><p>![image-20240812152215411](D:\Program Files (x86)\Typora\img-data\image-20240812152215411.png)</p>
<p>和MHA(multi-head attention)相似，receive Q K V,它在进行注意力机制之前减少了K和V的空间尺度，显著降低了计算和存贮</p>
<p>![image-20240814091739353](D:\Program Files (x86)\Typora\img-data\image-20240814091739353.png)</p>
<p>这里和原始transformer一样，只不过K和V先经过了一个空间降低操作。</p>
<p>![image-20240814091832785](D:\Program Files (x86)\Typora\img-data\image-20240814091832785.png)</p>
<p>![image-20240814091918147](D:\Program Files (x86)\Typora\img-data\image-20240814091918147.png)</p>
<p>hyper parameters:</p>
<p>$P_i$: patch size of Stage $i$</p>
<p>$C_i$: channel number of the output of Stage $i$</p>
<p>$L_i$: encoder layers number in Stage $i$</p>
<p>$R_i$: reduction ratio of the SRA in Stage $i$</p>
<p>$N_i$: head number of the SRA in Stage $i$</p>
<p>$E_i$: expansion ratio of the feed-forward layer in Stage $i$</p>
<p><strong>the rule of ResNet:</strong></p>
<ol>
<li><strong>use small output channel numbers in shallow stages</strong></li>
<li><strong>concentrate the major computation resource in intermediate stages.</strong></li>
<li><strong>with the growth of network depth, the hidden dimension gradually increases, and the output resolution progressively shrinks</strong></li>
<li><strong>the major computation resource is concentrated in Stage 3</strong></li>
</ol>
<p>the advantages over ViT:</p>
<ol>
<li>more flexible—can generate feature maps of different scales&#x2F;channels in different stages</li>
<li>more versatile(通用)—can be easily plugged and played in most downstream task models</li>
<li>more friendly to computation&#x2F;memory—can handle higher resolution feature maps or longer sequences</li>
</ol>
<p>Reference:</p>
<p><a href="https://blog.csdn.net/qq_38890412/article/details/120601834">全网最通俗易懂的 Self-Attention自注意力机制 讲解_self attention机制-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/weixin_42392454/article/details/122478544">狗都能看懂的Self-Attention讲解-CSDN博客</a></p>
<p><a href="https://www.bilibili.com/video/BV15v411W78M/?spm_id_from=333.788">https://www.bilibili.com/video/BV15v411W78M/?spm_id_from=333.788</a></p>
<p><a href="https://blog.csdn.net/oYeZhou/article/details/114288247">Pyramid Vision Transformer（PVT）: 纯Transformer设计，用于密集预测的通用backbone-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/weixin_73179708/article/details/132516512">从零开始了解transformer的机制|第四章：FFN层的作用-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/weixin_51756104/article/details/127250190">对Transformer中FeedForward层的理解_feedforward层的作用-CSDN博客</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>论文学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Mask-TS and ODC-SA net</title>
    <url>/undefined</url>
    <content><![CDATA[<h1 id="Mask-TS"><a href="#Mask-TS" class="headerlink" title="Mask-TS"></a>Mask-TS</h1><p>![image-20240705115352035](D:\Program Files (x86)\Typora\img-data\image-20240705115352035.png)</p>
<p>将原图、模型输出logits、概率图、不确定性图分别输入到4个一样的卷积网络当中，4个通过卷积之后代表它们考虑了像素点之间的位置关系。**（只要通过了卷积网络就已经考虑了像素之间的位置关系）**这四个卷积的输出放入一个注意力通道当中，通过自适应权重进行相加。</p>
<p>Hi和Pi这两个分支分别在增强边缘形状和更仔细地处理预测的真值方面发挥了作用，而x i包含丰富的原始信息，z i拥有与分割有关的特征，因为它源于一个分割网络</p>
<p>被认为是背景的部分就保持原始全局T0不变，被认为是病变区域的就用校准之后pre-output T‘。</p>
<h1 id="Mask-loss"><a href="#Mask-loss" class="headerlink" title="Mask-loss"></a>Mask-loss</h1><p>![image-20240705183615243](D:\Program Files (x86)\Typora\img-data\image-20240705183615243.png)</p>
<p>![image-20240705183653977](D:\Program Files (x86)\Typora\img-data\image-20240705183653977.png)</p>
<p>先将label和prediction合并，然后将calibrated logits的背景视为白色，忽略掉背景，将prediction黑白反转。再将两图合并，得到一个关注边缘的图像。再利用的二分类交叉熵损失函数。</p>
<h2 id="confidence-and-accuracy"><a href="#confidence-and-accuracy" class="headerlink" title="confidence and accuracy"></a>confidence and accuracy</h2><p>In learning algorithm, Confidence defines the probability of the event (or probability of input to fall in different classes). </p>
<p><strong>If a class has high probability then it has high confidence</strong>. Confidence value can be calculated for single input as well giving the meaning as how much the algorithm is confident for that class.</p>
<p>On the other hand, <strong>accuracy defines the skill of the learning algorithm to predict accurately.</strong> It defines the percentage of correct predictions made from all predictions.</p>
<h2 id="ECE是越小越好"><a href="#ECE是越小越好" class="headerlink" title="ECE是越小越好"></a>ECE是越小越好</h2><h1 id="ODC-SA-Net"><a href="#ODC-SA-Net" class="headerlink" title="ODC-SA Net"></a>ODC-SA Net</h1><p>to deal with multi-directional features and drastic changes in scale.多方向的特征和规模的急剧变化。</p>
<h2 id="existing-problems"><a href="#existing-problems" class="headerlink" title="existing problems"></a>existing problems</h2><ol>
<li><p>In the target images containing extremely hidden polyps, the angles of them change randomly with the movement of the lens, which makes the features of the polyps no longer representative in a fixed or specific few directions.在含有极度隐藏的息肉的目标图像中，它们的角度随着镜头的移动而随机变化，这使得息肉的特征不再代表固定或特定的几个方向。</p>
</li>
<li><p>The size of the hidden object in the polyp image varies dramatically息肉图像中隐藏物体的大小变化很大</p>
</li>
<li><p>Most of the existing methods ignore the importance of feature reorganization after the multi-layer feature fusion operation for the segmentation problem, which makes the rich semantic information obtained by the encoder cannot be fully utilized.现有的方法大多忽略了多层特征融合操作后的特征重组对分割问题的重要性，这使得编码器获得的丰富语义信息不能得到充分的利用。</p>
</li>
</ol>
<p>![image-20240714165544654](D:\Program Files (x86)\Typora\img-data\image-20240714165544654.png)</p>
<h2 id="ODC：Orthogonal-Direction-Enhancement-正交方向增强"><a href="#ODC：Orthogonal-Direction-Enhancement-正交方向增强" class="headerlink" title="ODC：Orthogonal Direction Enhancement  正交方向增强"></a>ODC：Orthogonal Direction Enhancement  正交方向增强</h2><blockquote>
<p>正交方向卷积（ODC）块可以通过形成正交特征向量基础，利用转置的矩形卷积核提取多方向特征，解决了随机特征方向变化的问题，减少了计算负荷。</p>
</blockquote>
<p>![image-20240715142552391](D:\Program Files (x86)\Typora\img-data\image-20240715142552391.png)</p>
<h3 id="需要解决的问题："><a href="#需要解决的问题：" class="headerlink" title="需要解决的问题："></a>需要解决的问题：</h3><p>different location and shooting direction of the colonoscopy-lens, the usual convolution kernel will miss features in specific directions due to the rotation of the image or the target because of its square shape and fixed sliding direction</p>
<p>由于结肠镜-透镜的不同位置和拍摄方向，通常的卷积核会因为图像或目标的旋转而错过特定方向的特征，因为它是方形的，而且滑动方向固定。</p>
<p>solves the problem of random changes in target feature directions？</p>
<h3 id="解决方法："><a href="#解决方法：" class="headerlink" title="解决方法："></a>解决方法：</h3><p>一对转置矩形卷积核（1×3和3×1）分别提取最深层图像的每个通道的垂直和水平的特征来形成一个正交特征基准</p>
<p>每个通道特征可以用一个线性表达式来表示:<br>$$<br>\pmb{f}^k &#x3D; a_k\pmb{r}^k + b_k\pmb{c}^k &#x3D; a_kr^k\pmb{i} + b_kc^k\pmb{j}<br>$$</p>
<p>网络下一层的任何通道的特征图应该是上一层的所有通道与任何方向的特征的组合</p>
<p>![image-20240715142428834](D:\Program Files (x86)\Typora\img-data\image-20240715142428834.png)</p>
<p>具体操作：</p>
<p>$F_{i}^{4}\in\mathbb{R}^{\frac{H}{32}\times\frac{W}{32}\times32}$  </p>
<p> $\pmb{r}<em>{i}\in\mathbb{R}^{\frac{H}{32}\times\frac{W}{32}\times32}   +\pmb{c}</em>{i}\in\mathbb{R}^{\frac{H}{32}\times\frac{W}{32}\times32}\rightarrow\pmb{h}_{i}\in\mathbb{R}^{\frac{H}{32}\times\frac{W}{32}\times128}$   ?为啥（我只能理解为它用了padding）</p>
<p>一个三层的1×3卷积去提取水平特征$$r_i$$ 另一个三层的3×1的卷积去提取垂直特征$$c_i$$</p>
<p>然后将这两个特征通过线性组合合并得到$h_i$</p>
<p>最后再进行一个残差连接，把原图接进来，得到$\pmb{Q}_{i}\in\mathbb{R}^{\frac{H}{32}\times\frac{W}{32}\times32}$</p>
<p>​	</p>
<h2 id="SA-Scale-Aware-扩大感受野-MSFA"><a href="#SA-Scale-Aware-扩大感受野-MSFA" class="headerlink" title="SA: Scale Aware  扩大感受野 MSFA"></a>SA: Scale Aware  扩大感受野 MSFA</h2><blockquote>
<p>Multi-scale Fusion Attention mechanism  多尺度融合注意（MSFA）机制 </p>
<p>emphasize scale changes in both spatial and channel dimensions, enhancing the segmentation accuracy for polyps of varying size 强调了空间和通道维度上的尺度变化，提高了不同大小的息肉的分割精度。</p>
</blockquote>
<p>![image-20240724171951166](D:\Program Files (x86)\Typora\img-data\image-20240724171951166.png)</p>
<h3 id="需要解决的问题：-1"><a href="#需要解决的问题：-1" class="headerlink" title="需要解决的问题："></a>需要解决的问题：</h3><p>The growth time of polyps in the intestine, the environment and the shooting distance of the colonoscopy lens are different. The polyps contained in the pictures have different sizes, and their color and texture characteristics are very similar to the background.</p>
<p>息肉在肠道、环境中的生长时间和结肠镜镜的拍摄距离是不同的。图片中包含的息肉有不同的尺寸，它们的颜色和纹理特征与背景非常相似。</p>
<p>Most of the existing methods treat scale-varying features as the same and ignore the attention bias caused by their differences in spatial and channel dimensions.</p>
<p>大多数现有的方法将标度变化的特征视为相同，而忽略了由它们在空间和通道维度上的差异引起的注意力偏差。</p>
<h3 id="解决方法：-1"><a href="#解决方法：-1" class="headerlink" title="解决方法："></a>解决方法：</h3><p>提出Spatial Scalable Enhancement Mechanism (S2E) and Channel Scalable Attention Mechanism (CSA)</p>
<p>S2E增强多尺度的空间特征，CSA在通道维度增强多尺度注意力</p>
<p>expand the receptive field in both spatial and channel dimensions by convolutional and pooling parallel dual channels of the high-level feature map Q i containing more semantic information, and use the feature map of the lower layer x3 i , which has more detail information than Q i , to give attention to the deep semantics of objects with different sizes to de-emphasize the influence of background and interference</p>
<p>通过卷积和池化并行的双通道方式扩大高等级特征图$$Q_i$$在空间和通道两个维度上的感受野，并使用包含更多详细信息的底层特征的$$x^3_i$$增加对于深层语义分割不同大小的注意力，降低对背景的关注。</p>
<h3 id="S2E：Spatial-Scalable-Enhancement-Mechanism-空间可扩展的增强机制"><a href="#S2E：Spatial-Scalable-Enhancement-Mechanism-空间可扩展的增强机制" class="headerlink" title="S2E：Spatial Scalable Enhancement Mechanism 空间可扩展的增强机制"></a>S2E：Spatial Scalable Enhancement Mechanism 空间可扩展的增强机制</h3><p>![image-20240724180802044](D:\Program Files (x86)\Typora\img-data\image-20240724180802044.png)</p>
<p>双线并行</p>
<p>一个是包含3个3*3的卷积层，可以获取局部特征信息并通过卷积操作增强其对于小物体的特征信息。</p>
<p>另一个是一个平均池化层，在一些特定区域（2*2大小），平均池化可以保留大目标，减弱小目标（由于更大的目标包含更多的像素点），这样之后新特征图的总体分布和大目标的分布更接近，同时也获得了全局的上下文信息。</p>
<p>它减弱了空间分辨率但增强了全局上下文信息在没有增大计算量的前提下。</p>
<p>在平均池化之后，插值被放大（没有对小物体的削弱进行补偿）来保证和第一个通道上的图片大小一致，并且相加。</p>
<p>这样可以做到融合了空间多尺度信息，同时不用很大的计算量。</p>
<p>Output：$$S_i$$</p>
<h3 id="CSA：Channel-Scalable-Attention-Mechanism-通道可扩展注意机制"><a href="#CSA：Channel-Scalable-Attention-Mechanism-通道可扩展注意机制" class="headerlink" title="CSA：Channel Scalable Attention Mechanism 通道可扩展注意机制"></a>CSA：Channel Scalable Attention Mechanism 通道可扩展注意机制</h3><p>![image-20240724220323241](D:\Program Files (x86)\Typora\img-data\image-20240724220323241.png)</p>
<p>要解决的问题：Different channels contain different semantic information, and there are scale differences in the targets mapped by these channels.不同的通道包含不同的语义信息，这些通道映射的目标也存在规模差异。</p>
<p>对于小目标：传统的 Point-Wise-Conv + ReLU 提升注意力</p>
<p>对于大目标：先进行平均池化之后再进行 Point-Wise-Conv + ReLU 利用每个通道特征图的像素平均值，在全局信息的引导下获得通道注意力，增加了大尺度特征图的权重</p>
<p>两个分支的输出相加送入Sigmiod得到输出$$W_i$$，之后和$$S_i$$相点乘之后再和$$Q_i$$相加得到$$D_i$$   ?学长好像写错了，应该是先和Si点乘之后再和Qi相加</p>
<p>Output：$$D_i$$</p>
<h3 id="RFA：Residual-Fusion-Attention-剩余融合注意"><a href="#RFA：Residual-Fusion-Attention-剩余融合注意" class="headerlink" title="RFA：Residual Fusion Attention  剩余融合注意"></a>RFA：Residual Fusion Attention  剩余融合注意</h3><p>![image-20240726223106998](D:\Program Files (x86)\Typora\img-data\image-20240726223106998.png)</p>
<p>$$F^3_i$$来指导$$D_i$$进行浅层和深层特征融合。</p>
<p>$$D_i$$经过上采样和驻点卷积使得和$$F^3_i$$大小一样，同样$$F^3_i$$也经过逐点卷积得到相同大小，相加之后经过一个ReLU激活函数和一个Conv block ，之后再进行一次残差连接，最后的通过softmax输出的$$E_i$$包含了每个像素的有效信息。最终$$E_i$$再和上采样之后的$$D_i$$点乘，自动关注目标区域。</p>
<p>Output:$$C_i$$</p>
<h2 id="Extraction-with-Re-attention-Modul-ERA"><a href="#Extraction-with-Re-attention-Modul-ERA" class="headerlink" title="Extraction with Re-attention Modul (ERA)"></a>Extraction with Re-attention Modul (ERA)</h2><blockquote>
<p>re-combine effective feature 重新组合有效的特征</p>
</blockquote>
<p>两组通道和空间注意机制用来提取经过多层特征融合之后的深度语义信息。特征重新提取和权重分配更好地利用语义信息并防止细节被忽略。</p>
<p>![image-20240727102332424](D:\Program Files (x86)\Typora\img-data\image-20240727102332424.png)</p>
<p>further adjust and integrate its own feature information. This process generates a feature enhancement and combination graph that is more in line with the target. It allows different features to be combined into better features and can also realize the allocation of different feature emphasis degrees.</p>
<p>进一步调整和整合自己的特征信息。这个过程产生了一个更符合目标的特征增强和组合图。它允许不同的特征被组合成更好的特征，也可以实现不同特征强调程度的分配。</p>
<p>实现更好的拟合。</p>
<p>Output:$$\widehat{Z_i}$$</p>
<h2 id="Shallow-Reverse-Attention-Mechanism-SRA"><a href="#Shallow-Reverse-Attention-Mechanism-SRA" class="headerlink" title="Shallow Reverse Attention Mechanism (SRA)"></a>Shallow Reverse Attention Mechanism (SRA)</h2><blockquote>
<p>enhance polyp edge with low level information 用低层次的信息增强息肉边缘</p>
</blockquote>
<p>渐层的编码输出有较多的边缘信息，指导RA（Reverse Attention）增强分割目标。</p>
<p>![image-20240727105341404](D:\Program Files (x86)\Typora\img-data\image-20240727105341404.png)</p>
<p>problems:</p>
<p>the boundary between the background and the target is not obvious</p>
<p>solutions:</p>
<p>Inspired by ParNet, introduce RA (RA exploits the details of complementary regions by erasing the polyp regions that have been judged RA通过擦除已经判断过的息肉区域来利用互补区域的细节)</p>
<p>我们的网络使用最浅层的特征图来进行边界增强。</p>
<p>Output: $$\hat{p_i}$$</p>
<h2 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h2><p>![image-20240727110943522](D:\Program Files (x86)\Typora\img-data\image-20240727110943522.png)</p>
<p>![image-20240727110950672](D:\Program Files (x86)\Typora\img-data\image-20240727110950672.png)</p>
<p>使用deep supervision $$z_i$$上采样到和$$G$$一样大，$$\hat{p_i}$$也一样。</p>
<h1 id="Reference："><a href="#Reference：" class="headerlink" title="Reference："></a>Reference：</h1><p><a href="https://blog.csdn.net/qq_39478403/article/details/121181904">【机器学习】详解 转置卷积 (Transpose Convolution)-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/yx868yx/article/details/107158692">增强感受野SPP、ASPP、RFB、PPM_spp模块的作用-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/weixin_39653948/article/details/104621249">markdown中的特殊字符、数学公式、图表等语法总结_markdown 制作流程图 有特殊符号-CSDN博客</a></p>
<p>[markdown公式中字母加粗_markdown 公式加粗-CSDN博客](<a href="https://blog.csdn.net/weixin_43964993/article/details/108360029#:~:text=%E5%8A%A0%E7%B2%97%EF%BC%9A">https://blog.csdn.net/weixin_43964993/article/details/108360029#:~:text=加粗：</a> %24pmb {字母}%24,加粗倾斜： %24boldsymbol {字母}%24)</p>
<p><a href="https://blog.csdn.net/Ying_Xu/article/details/51240291">Latex所有常用数学符号整理_latex数学符号-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/tintinetmilou/article/details/81607721">Depthwise卷积与Pointwise卷积-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/m0_37605642/article/details/135958384">通俗易懂理解注意力机制(Attention Mechanism)-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/oYeZhou/article/details/114288247">Pyramid Vision Transformer（PVT）: 纯Transformer设计，用于密集预测的通用backbone-CSDN博客</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>论文学习</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode</title>
    <url>/undefined</url>
    <content><![CDATA[<h1 id="Leetcode-704-二分查找-24-12-13"><a href="#Leetcode-704-二分查找-24-12-13" class="headerlink" title="Leetcode  704. 二分查找 24.12.13"></a>Leetcode  <a href="https://leetcode.cn/problems/binary-search/">704. 二分查找</a> 24.12.13</h1><h2 id="梦开始的地方"><a href="#梦开始的地方" class="headerlink" title="梦开始的地方"></a>梦开始的地方</h2><h2 id="题面"><a href="#题面" class="headerlink" title="题面"></a>题面</h2><p>给定一个 <code>n</code> 个元素有序的（升序）整型数组 <code>nums</code> 和一个目标值 <code>target</code> ，写一个函数搜索 <code>nums</code> 中的 <code>target</code>，如果目标值存在返回下标，否则返回 <code>-1</code>。</p>
<p><strong>示例 1:</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入: nums = [-1,0,3,5,9,12], target = 9</span><br><span class="line">输出: 4</span><br><span class="line">解释: 9 出现在 nums 中并且下标为 4</span><br></pre></td></tr></table></figure>

<p><strong>示例 2:</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入: nums = [-1,0,3,5,9,12], target = 2</span><br><span class="line">输出: -1</span><br><span class="line">解释: 2 不存在 nums 中因此返回 -1</span><br></pre></td></tr></table></figure>

<p><strong>提示：</strong></p>
<ol>
<li>你可以假设 <code>nums</code> 中的所有元素是不重复的。</li>
<li><code>n</code> 将在 <code>[1, 10000]</code>之间。</li>
<li><code>nums</code> 的每个元素都将在 <code>[-9999, 9999]</code>之间。</li>
</ol>
<h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">search</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> left=<span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> right=nums.<span class="built_in">size</span>()<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">while</span> (left&lt;=right)&#123;</span><br><span class="line">            <span class="type">int</span> middle=(left+right)/<span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span> (nums[middle]&lt;target)&#123;</span><br><span class="line">                left=middle<span class="number">+1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (nums[middle]&gt;target)&#123;</span><br><span class="line">                right=middle<span class="number">-1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">return</span> middle;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<h1 id="Leetcode-27-移除元素-24-12-15"><a href="#Leetcode-27-移除元素-24-12-15" class="headerlink" title="Leetcode 27. 移除元素 24.12.15"></a>Leetcode <a href="https://leetcode.cn/problems/remove-element/">27. 移除元素</a> 24.12.15</h1><h2 id="题面-1"><a href="#题面-1" class="headerlink" title="题面"></a>题面</h2><p>给你一个数组 <code>nums</code> 和一个值 <code>val</code>，你需要 <strong><a href="https://baike.baidu.com/item/%E5%8E%9F%E5%9C%B0%E7%AE%97%E6%B3%95">原地</a></strong> 移除所有数值等于 <code>val</code> 的元素。元素的顺序可能发生改变。然后返回 <code>nums</code> 中与 <code>val</code> 不同的元素的数量。</p>
<p>假设 <code>nums</code> 中不等于 <code>val</code> 的元素数量为 <code>k</code>，要通过此题，您需要执行以下操作：</p>
<ul>
<li>更改 <code>nums</code> 数组，使 <code>nums</code> 的前 <code>k</code> 个元素包含不等于 <code>val</code> 的元素。<code>nums</code> 的其余元素和 <code>nums</code> 的大小并不重要。</li>
<li>返回 <code>k</code>。</li>
</ul>
<p><strong>用户评测：</strong></p>
<p>评测机将使用以下代码测试您的解决方案：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int[] nums = [...]; // 输入数组</span><br><span class="line">int val = ...; // 要移除的值</span><br><span class="line">int[] expectedNums = [...]; // 长度正确的预期答案。</span><br><span class="line">                            // 它以不等于 val 的值排序。</span><br><span class="line"></span><br><span class="line">int k = removeElement(nums, val); // 调用你的实现</span><br><span class="line"></span><br><span class="line">assert k == expectedNums.length;</span><br><span class="line">sort(nums, 0, k); // 排序 nums 的前 k 个元素</span><br><span class="line">for (int i = 0; i &lt; actualLength; i++) &#123;</span><br><span class="line">    assert nums[i] == expectedNums[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如果所有的断言都通过，你的解决方案将会 <strong>通过</strong>。</p>
<p><strong>示例 1：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [3,2,2,3], val = 3</span><br><span class="line">输出：2, nums = [2,2,_,_]</span><br><span class="line">解释：你的函数函数应该返回 k = 2, 并且 nums 中的前两个元素均为 2。</span><br><span class="line">你在返回的 k 个元素之外留下了什么并不重要（因此它们并不计入评测）。</span><br></pre></td></tr></table></figure>

<p><strong>示例 2：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [0,1,2,2,3,0,4,2], val = 2</span><br><span class="line">输出：5, nums = [0,1,4,0,3,_,_,_]</span><br><span class="line">解释：你的函数应该返回 k = 5，并且 nums 中的前五个元素为 0,0,1,3,4。</span><br><span class="line">注意这五个元素可以任意顺序返回。</span><br><span class="line">你在返回的 k 个元素之外留下了什么并不重要（因此它们并不计入评测）。</span><br></pre></td></tr></table></figure>

<p><strong>提示：</strong></p>
<ul>
<li><code>0 &lt;= nums.length &lt;= 100</code></li>
<li><code>0 &lt;= nums[i] &lt;= 50</code></li>
<li><code>0 &lt;= val &lt;= 100</code></li>
</ul>
<h2 id="题解：-双指针"><a href="#题解：-双指针" class="headerlink" title="题解： 双指针"></a>题解： 双指针</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">removeElement</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums, <span class="type">int</span> val)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> slow=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> fast=<span class="number">0</span>;fast&lt;nums.<span class="built_in">size</span>();fast++)&#123;</span><br><span class="line">            <span class="keyword">if</span> (nums[fast]!=val)&#123;</span><br><span class="line">                nums[slow++]=nums[fast];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> slow;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<h1 id="Leetcode-977-有序数组的平方-24-12-15"><a href="#Leetcode-977-有序数组的平方-24-12-15" class="headerlink" title="Leetcode 977. 有序数组的平方 24.12.15"></a>Leetcode <a href="https://leetcode.cn/problems/squares-of-a-sorted-array/">977. 有序数组的平方</a> 24.12.15</h1><h2 id="题面-2"><a href="#题面-2" class="headerlink" title="题面"></a>题面</h2><p>给你一个按 <strong>非递减顺序</strong> 排序的整数数组 <code>nums</code>，返回 <strong>每个数字的平方</strong> 组成的新数组，要求也按 <strong>非递减顺序</strong> 排序。</p>
<p><strong>示例 1：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [-4,-1,0,3,10]</span><br><span class="line">输出：[0,1,9,16,100]</span><br><span class="line">解释：平方后，数组变为 [16,1,0,9,100]</span><br><span class="line">排序后，数组变为 [0,1,9,16,100]</span><br></pre></td></tr></table></figure>

<p><strong>示例 2：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：nums = [-7,-3,2,3,11]</span><br><span class="line">输出：[4,9,9,49,121]</span><br></pre></td></tr></table></figure>

<p><strong>提示：</strong></p>
<ul>
<li><code>1 &lt;= nums.length &lt;= 104</code></li>
<li><code>-104 &lt;= nums[i] &lt;= 104</code></li>
<li><code>nums</code> 已按 <strong>非递减顺序</strong> 排序</li>
</ul>
<p><strong>进阶：</strong></p>
<ul>
<li>请你设计时间复杂度为 <code>O(n)</code> 的算法解决本问题</li>
</ul>
<h2 id="题解：双指针，注意初始化vector长度"><a href="#题解：双指针，注意初始化vector长度" class="headerlink" title="题解：双指针，注意初始化vector长度"></a>题解：双指针，注意初始化vector长度</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">sortedSquares</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">res</span><span class="params">(nums.size())</span></span>;</span><br><span class="line">        <span class="type">int</span> t=nums.<span class="built_in">size</span>()<span class="number">-1</span>;</span><br><span class="line">        <span class="type">int</span> i,j;</span><br><span class="line">        <span class="keyword">for</span> (i=<span class="number">0</span>, j=nums.<span class="built_in">size</span>()<span class="number">-1</span>;i&lt;=j;)&#123;</span><br><span class="line">            <span class="keyword">if</span> ( <span class="built_in">abs</span>(nums[i]) &gt; <span class="built_in">abs</span>(nums[j]) ) &#123;</span><br><span class="line">                res[t]=nums[i]*nums[i];</span><br><span class="line">                <span class="comment">//res.insert(res.begin(),nums[i]*nums[i]);</span></span><br><span class="line">                i++;t--;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                res[t]=nums[j]*nums[j];</span><br><span class="line">                <span class="comment">//res.insert(res.begin(),nums[j]*nums[j]);</span></span><br><span class="line">                j--;t--;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<h1 id="Leetcode-209-长度最小的子数组-24-12-17"><a href="#Leetcode-209-长度最小的子数组-24-12-17" class="headerlink" title="Leetcode 209. 长度最小的子数组 24.12.17"></a>Leetcode <a href="https://leetcode.cn/problems/minimum-size-subarray-sum/">209. 长度最小的子数组</a> 24.12.17</h1><h2 id="题面-3"><a href="#题面-3" class="headerlink" title="题面"></a>题面</h2><p>给定一个含有 <code>n</code> 个正整数的数组和一个正整数 <code>target</code> <strong>。</strong></p>
<p>找出该数组中满足其总和大于等于 <code>target</code> 的长度最小的 </p>
<p><strong>子数组</strong></p>
<p><code>[numsl, numsl+1, ..., numsr-1, numsr]</code> ，并返回其长度**。**如果不存在符合条件的子数组，返回 <code>0</code> 。</p>
<p><strong>示例 1：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：target = 7, nums = [2,3,1,2,4,3]</span><br><span class="line">输出：2</span><br><span class="line">解释：子数组 [4,3] 是该条件下的长度最小的子数组。</span><br></pre></td></tr></table></figure>

<p><strong>示例 2：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：target = 4, nums = [1,4,4]</span><br><span class="line">输出：1</span><br></pre></td></tr></table></figure>

<p><strong>示例 3：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：target = 11, nums = [1,1,1,1,1,1,1,1]</span><br><span class="line">输出：0</span><br></pre></td></tr></table></figure>

<p><strong>提示：</strong></p>
<ul>
<li><code>1 &lt;= target &lt;= 109</code></li>
<li><code>1 &lt;= nums.length &lt;= 105</code></li>
<li><code>1 &lt;= nums[i] &lt;= 104</code></li>
</ul>
<p><strong>进阶：</strong></p>
<ul>
<li>如果你已经实现 <code>O(n)</code> 时间复杂度的解法, 请尝试设计一个 <code>O(n log(n))</code> 时间复杂度的解法。</li>
</ul>
<h2 id="题解：滑动窗口"><a href="#题解：滑动窗口" class="headerlink" title="题解：滑动窗口"></a>题解：滑动窗口</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">minSubArrayLen</span><span class="params">(<span class="type">int</span> target, vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> i=<span class="number">0</span>;<span class="comment">//起始</span></span><br><span class="line">        <span class="type">int</span> sum=<span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> res=<span class="number">99999999</span>;</span><br><span class="line">        <span class="type">int</span> len=<span class="number">0</span>;</span><br><span class="line">        <span class="comment">//j为终止下标</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j=<span class="number">0</span>;j&lt;nums.<span class="built_in">size</span>();j++)&#123;</span><br><span class="line">            sum+=nums[j];</span><br><span class="line">            <span class="keyword">while</span>(sum&gt;=target)&#123;</span><br><span class="line">                len=j-i<span class="number">+1</span>;</span><br><span class="line">                res=<span class="built_in">min</span>(res,len);</span><br><span class="line">                sum-=nums[i];</span><br><span class="line">                i++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (sum&lt;target&amp;&amp;res==<span class="number">99999999</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<h1 id="Leetcode-59-螺旋矩阵-II-24-12-17"><a href="#Leetcode-59-螺旋矩阵-II-24-12-17" class="headerlink" title="Leetcode 59. 螺旋矩阵 II  24.12.17"></a>Leetcode <a href="https://leetcode.cn/problems/spiral-matrix-ii/">59. 螺旋矩阵 II </a> 24.12.17</h1><p>给你一个正整数 <code>n</code> ，生成一个包含 <code>1</code> 到 <code>n2</code> 所有元素，且元素按顺时针顺序螺旋排列的 <code>n x n</code> 正方形矩阵 <code>matrix</code> 。</p>
<p><strong>示例 1：</strong></p>
<p><img src="https://assets.leetcode.com/uploads/2020/11/13/spiraln.jpg" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：n = 3</span><br><span class="line">输出：[[1,2,3],[8,9,4],[7,6,5]]</span><br></pre></td></tr></table></figure>

<p><strong>示例 2：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：n = 1</span><br><span class="line">输出：[[1]]</span><br></pre></td></tr></table></figure>

<p><strong>提示：</strong></p>
<ul>
<li><code>1 &lt;= n &lt;= 20</code></li>
</ul>
<h2 id="题解：模拟，注意创建二维数组方式"><a href="#题解：模拟，注意创建二维数组方式" class="headerlink" title="题解：模拟，注意创建二维数组方式"></a>题解：模拟，注意创建二维数组方式</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">generateMatrix</span>(<span class="type">int</span> n) &#123;</span><br><span class="line">        <span class="comment">//采用左闭右开</span></span><br><span class="line">        vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">matrix</span>(n,<span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(n));</span><br><span class="line">        <span class="type">int</span> startx=<span class="number">0</span>;<span class="type">int</span> starty=<span class="number">0</span>;<span class="comment">//起始位置</span></span><br><span class="line">        <span class="type">int</span> offset=<span class="number">1</span>;<span class="comment">//右开</span></span><br><span class="line">        <span class="type">int</span> t=n/<span class="number">2</span>;<span class="comment">//转几圈</span></span><br><span class="line">        <span class="type">int</span> i,j;</span><br><span class="line">        <span class="type">int</span> count=<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span>(t)&#123;</span><br><span class="line">            <span class="comment">//从左到右</span></span><br><span class="line">            <span class="keyword">for</span> (j=starty;j&lt;n-offset;j++)&#123;</span><br><span class="line">                matrix[startx][j]=count++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//从上到下</span></span><br><span class="line">            <span class="keyword">for</span> (i=startx;i&lt;n-offset;i++)&#123;</span><br><span class="line">                matrix[i][j]=count++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//从右到左</span></span><br><span class="line">            <span class="keyword">for</span> (;j&gt;starty;j--)&#123;</span><br><span class="line">                matrix[i][j]=count++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//从下到上</span></span><br><span class="line">            <span class="keyword">for</span> (;i&gt;startx;i--)&#123;</span><br><span class="line">                matrix[i][j]=count++;</span><br><span class="line">            &#125;</span><br><span class="line">            startx++;starty++;offset++;</span><br><span class="line">            t--;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (n%<span class="number">2</span>==<span class="number">1</span>)&#123;</span><br><span class="line">            matrix[n/<span class="number">2</span>][n/<span class="number">2</span>]=count++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> matrix;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<h1 id="Leetcode-203-移除链表元素"><a href="#Leetcode-203-移除链表元素" class="headerlink" title="Leetcode 203. 移除链表元素"></a>Leetcode <a href="https://leetcode.cn/problems/remove-linked-list-elements/">203. 移除链表元素</a></h1><h2 id="题面-4"><a href="#题面-4" class="headerlink" title="题面"></a>题面</h2><p>给你一个链表的头节点 <code>head</code> 和一个整数 <code>val</code> ，请你删除链表中所有满足 <code>Node.val == val</code> 的节点，并返回 <strong>新的头节点</strong> 。</p>
<p><strong>示例 1：</strong></p>
<p><img src="https://assets.leetcode.com/uploads/2021/03/06/removelinked-list.jpg" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：head = [1,2,6,3,4,5,6], val = 6</span><br><span class="line">输出：[1,2,3,4,5]</span><br></pre></td></tr></table></figure>

<p><strong>示例 2：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：head = [], val = 1</span><br><span class="line">输出：[]</span><br></pre></td></tr></table></figure>

<p><strong>示例 3：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：head = [7,7,7,7], val = 7</span><br><span class="line">输出：[]</span><br></pre></td></tr></table></figure>

<p><strong>提示：</strong></p>
<ul>
<li>列表中的节点数目在范围 <code>[0, 104]</code> 内</li>
<li><code>1 &lt;= Node.val &lt;= 50</code></li>
<li><code>0 &lt;= val &lt;= 50</code></li>
</ul>
<h2 id="题解：虚拟头节点"><a href="#题解：虚拟头节点" class="headerlink" title="题解：虚拟头节点"></a>题解：虚拟头节点</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"> * struct ListNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     ListNode *next;</span></span><br><span class="line"><span class="comment"> *     ListNode() : val(0), next(nullptr) &#123;&#125;</span></span><br><span class="line"><span class="comment"> *     ListNode(int x) : val(x), next(nullptr) &#123;&#125;</span></span><br><span class="line"><span class="comment"> *     ListNode(int x, ListNode *next) : val(x), next(next) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//方法一：虚拟头节点，这种方法比较正常</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">removeElements</span><span class="params">(ListNode* head, <span class="type">int</span> val)</span> </span>&#123;</span><br><span class="line">        ListNode* dhead= <span class="keyword">new</span> ListNode;</span><br><span class="line">        dhead-&gt;next=head;</span><br><span class="line">        ListNode* cur=dhead;</span><br><span class="line">        <span class="keyword">while</span>(cur-&gt;next!=<span class="literal">NULL</span>)&#123;</span><br><span class="line">            <span class="keyword">if</span> (cur-&gt;next-&gt;val==val)&#123;</span><br><span class="line">                cur-&gt;next=cur-&gt;next-&gt;next;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                cur=cur-&gt;next;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dhead-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//方法二：不创建虚拟头节点，直接使用第一个元素作为head，需要多处理一下删除头节点的情况，相对复杂</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">removeElements</span><span class="params">(ListNode* head, <span class="type">int</span> val)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (head!=<span class="literal">NULL</span>&amp;&amp;head-&gt;val==val)&#123;</span><br><span class="line">            head=head-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        ListNode* cur=head;</span><br><span class="line">        <span class="keyword">if</span> (cur==<span class="literal">NULL</span>) <span class="keyword">return</span> head;</span><br><span class="line">        <span class="keyword">while</span>(cur-&gt;next!=<span class="literal">NULL</span>&amp;&amp;cur!=<span class="literal">NULL</span>)&#123;</span><br><span class="line">            <span class="keyword">if</span> (cur-&gt;next-&gt;val==val)&#123;</span><br><span class="line">                cur-&gt;next=cur-&gt;next-&gt;next;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                cur=cur-&gt;next;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> head;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>





<h1 id="Leetcode-707-设计链表-1-12"><a href="#Leetcode-707-设计链表-1-12" class="headerlink" title="Leetcode 707. 设计链表 1.12"></a>Leetcode <a href="https://leetcode.cn/problems/design-linked-list/">707. 设计链表 </a>1.12</h1><h1 id="题面-5"><a href="#题面-5" class="headerlink" title="题面"></a>题面</h1><p>你可以选择使用单链表或者双链表，设计并实现自己的链表。</p>
<p>单链表中的节点应该具备两个属性：<code>val</code> 和 <code>next</code> 。<code>val</code> 是当前节点的值，<code>next</code> 是指向下一个节点的指针&#x2F;引用。</p>
<p>如果是双向链表，则还需要属性 <code>prev</code> 以指示链表中的上一个节点。假设链表中的所有节点下标从 <strong>0</strong> 开始。</p>
<p>实现 <code>MyLinkedList</code> 类：</p>
<ul>
<li><code>MyLinkedList()</code> 初始化 <code>MyLinkedList</code> 对象。</li>
<li><code>int get(int index)</code> 获取链表中下标为 <code>index</code> 的节点的值。如果下标无效，则返回 <code>-1</code> 。</li>
<li><code>void addAtHead(int val)</code> 将一个值为 <code>val</code> 的节点插入到链表中第一个元素之前。在插入完成后，新节点会成为链表的第一个节点。</li>
<li><code>void addAtTail(int val)</code> 将一个值为 <code>val</code> 的节点追加到链表中作为链表的最后一个元素。</li>
<li><code>void addAtIndex(int index, int val)</code> 将一个值为 <code>val</code> 的节点插入到链表中下标为 <code>index</code> 的节点之前。如果 <code>index</code> 等于链表的长度，那么该节点会被追加到链表的末尾。如果 <code>index</code> 比长度更大，该节点将 <strong>不会插入</strong> 到链表中。</li>
<li><code>void deleteAtIndex(int index)</code> 如果下标有效，则删除链表中下标为 <code>index</code> 的节点。</li>
</ul>
<p><strong>示例：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入</span><br><span class="line">[&quot;MyLinkedList&quot;, &quot;addAtHead&quot;, &quot;addAtTail&quot;, &quot;addAtIndex&quot;, &quot;get&quot;, &quot;deleteAtIndex&quot;, &quot;get&quot;]</span><br><span class="line">[[], [1], [3], [1, 2], [1], [1], [1]]</span><br><span class="line">输出</span><br><span class="line">[null, null, null, null, 2, null, 3]</span><br><span class="line"></span><br><span class="line">解释</span><br><span class="line">MyLinkedList myLinkedList = new MyLinkedList();</span><br><span class="line">myLinkedList.addAtHead(1);</span><br><span class="line">myLinkedList.addAtTail(3);</span><br><span class="line">myLinkedList.addAtIndex(1, 2);    // 链表变为 1-&gt;2-&gt;3</span><br><span class="line">myLinkedList.get(1);              // 返回 2</span><br><span class="line">myLinkedList.deleteAtIndex(1);    // 现在，链表变为 1-&gt;3</span><br><span class="line">myLinkedList.get(1);              // 返回 3</span><br></pre></td></tr></table></figure>

<p><strong>提示：</strong></p>
<ul>
<li><code>0 &lt;= index, val &lt;= 1000</code></li>
<li>请不要使用内置的 LinkedList 库。</li>
<li>调用 <code>get</code>、<code>addAtHead</code>、<code>addAtTail</code>、<code>addAtIndex</code> 和 <code>deleteAtIndex</code> 的次数不超过 <code>2000</code> 。</li>
</ul>
<h2 id="题解：挺红的，这给一个样例改一次bug，改了有半小时还不止，关注addAtTail，addAtIndex，deleteAtIndex函数，它们都从head开始遍历，不从head-next开始"><a href="#题解：挺红的，这给一个样例改一次bug，改了有半小时还不止，关注addAtTail，addAtIndex，deleteAtIndex函数，它们都从head开始遍历，不从head-next开始" class="headerlink" title="题解：挺红的，这给一个样例改一次bug，改了有半小时还不止，关注addAtTail，addAtIndex，deleteAtIndex函数，它们都从head开始遍历，不从head-&gt;next开始"></a>题解：挺红的，这给一个样例改一次bug，改了有半小时还不止，关注addAtTail，addAtIndex，deleteAtIndex函数，它们都从head开始遍历，不从head-&gt;next开始</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 题目当中的ListNode的结构是这样的</span></span><br><span class="line"><span class="comment">// struct ListNode&#123;</span></span><br><span class="line"><span class="comment">//     int val;</span></span><br><span class="line"><span class="comment">//     ListNode *next;</span></span><br><span class="line"><span class="comment">// &#125;;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinkedList</span> &#123;</span><br><span class="line"><span class="comment">//public:</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">int</span> size;</span><br><span class="line">    ListNode *head;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">MyLinkedList</span>() &#123;</span><br><span class="line">        size=<span class="number">0</span>;</span><br><span class="line">        head = <span class="keyword">new</span> ListNode;</span><br><span class="line">        head-&gt;val=<span class="number">0</span>;</span><br><span class="line">        head-&gt;next=<span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 第一个元素的下标是0，从虚拟头节点的下一个开始向后遍历</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">get</span><span class="params">(<span class="type">int</span> index)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (index&lt;<span class="number">0</span>||index&gt;size<span class="number">-1</span>) <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        ListNode *cur = head-&gt;next;</span><br><span class="line">        <span class="keyword">while</span> (index--)&#123;</span><br><span class="line">            cur=cur-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> cur-&gt;val;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">addAtHead</span><span class="params">(<span class="type">int</span> val)</span> </span>&#123;</span><br><span class="line">        ListNode *t=<span class="keyword">new</span> ListNode;</span><br><span class="line">        t-&gt;val=val;</span><br><span class="line">        t-&gt;next=head-&gt;next;</span><br><span class="line">        head-&gt;next=t;    </span><br><span class="line">        size++;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">addAtTail</span><span class="params">(<span class="type">int</span> val)</span> </span>&#123;</span><br><span class="line">        ListNode *t = <span class="keyword">new</span> ListNode;</span><br><span class="line">        ListNode *cur = head;</span><br><span class="line">        <span class="keyword">while</span> (cur-&gt;next!=<span class="literal">NULL</span>)&#123;</span><br><span class="line">            cur= cur-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        cur-&gt;next=t;</span><br><span class="line">        t-&gt;next=<span class="literal">NULL</span>;</span><br><span class="line">        t-&gt;val=val;</span><br><span class="line">        size++;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">addAtIndex</span><span class="params">(<span class="type">int</span> index, <span class="type">int</span> val)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (index&gt;size) <span class="keyword">return</span> ;</span><br><span class="line">        ListNode *t = <span class="keyword">new</span> ListNode;</span><br><span class="line">        ListNode *cur = head;</span><br><span class="line">        <span class="comment">//index-=1;</span></span><br><span class="line">        <span class="keyword">while</span> (index&gt;<span class="number">0</span>)&#123;</span><br><span class="line">            cur=cur-&gt;next;</span><br><span class="line">            index--;</span><br><span class="line">        &#125;</span><br><span class="line">        t-&gt;next=cur-&gt;next;</span><br><span class="line">        cur-&gt;next=t;</span><br><span class="line">        t-&gt;val=val;</span><br><span class="line">        size++;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">deleteAtIndex</span><span class="params">(<span class="type">int</span> index)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (index&lt;<span class="number">0</span>||index&gt;size<span class="number">-1</span>) <span class="keyword">return</span> ;</span><br><span class="line">        ListNode *cur=head;</span><br><span class="line">        <span class="comment">//index-=1;</span></span><br><span class="line">        <span class="keyword">while</span> (index&gt;<span class="number">0</span>)&#123;</span><br><span class="line">            cur=cur-&gt;next;</span><br><span class="line">            index--;</span><br><span class="line">        &#125;</span><br><span class="line">        cur-&gt;next=cur-&gt;next-&gt;next;</span><br><span class="line">        size--;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Your MyLinkedList object will be instantiated and called as such:</span></span><br><span class="line"><span class="comment"> * MyLinkedList* obj = new MyLinkedList();</span></span><br><span class="line"><span class="comment"> * int param_1 = obj-&gt;get(index);</span></span><br><span class="line"><span class="comment"> * obj-&gt;addAtHead(val);</span></span><br><span class="line"><span class="comment"> * obj-&gt;addAtTail(val);</span></span><br><span class="line"><span class="comment"> * obj-&gt;addAtIndex(index,val);</span></span><br><span class="line"><span class="comment"> * obj-&gt;deleteAtIndex(index);</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>算法学习</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>GPS</title>
    <url>/undefined</url>
    <content><![CDATA[<hr>
<p><a href="https://www.semanticscholar.org/paper/Greedy-Policy-Search%3A-A-Simple-Baseline-for-Molchanov-Lyzhov/8286fbad6d4712d950689930720134bc8828963f?sort=relevance&queryString=segmentation"> Greedy Policy Search: A Simple Baseline for Learnable Test-Time Augmentation | Semantic Scholar</a></p>
<p><a href="https://github.com/SamsungLabs/gps-augment?tab=readme-ov-file">SamsungLabs&#x2F;gps-augment: Simple but high-performing method for learning a policy of test-time augmentation (github.com)</a></p>
<p>在一个已经训练好的网络上将测试数据增广，利用这些数据来测量网络的不确定性</p>
<p>GPS（greedy policy search）：a simple algorithm that learns a policy for test-time data augmentation based on the predictive performance on a validation set. </p>
<p>一个简单的算法，<strong>根据验证集的预测性能</strong>来学习测试时间数据增强的策略。</p>
<p>In an ablation study, we show that <strong>optimizing the calibrated log-likelihood</strong><strong>（优化校准的对数似然）</strong> (Ashukha et al., 2020) is <strong>a crucial part</strong> of the policy search algorithm, while the default objectives—<strong>accuracy and loglikelihood—lead to a significant drop in the final performance.</strong></p>
<p>定义TTA policy为一系列子政策的集合，每个子政策又包含Ns个连续的图片变换。</p>
<p>步骤：</p>
<p>1、 先用CIFAR10数据集训练VGG模型，并用randaugment增广数据</p>
<p>2、 将这些增广好的数据进行预测处理并选取其中的一部分作为备选池。本文共选取1101个子政策</p>
<p>3、 用校准的对数似然来评价，选择让可以在聚合预测时提供最大提升的子政策进入pool中</p>
<p><strong>Prediction通过求不同子政策的平均得到</strong></p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image002-1736691190942-1.jpg)</p>
<p>GPS的核心算法（利用贪心和校准的对数似然）</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image004-1736691190944-2.png)</p>
<p>评判标准：校准的对数似然（对数似然加上temperature scaling）</p>
<p>GPS是一个学习型（learnable）的策略，它在过程中不断选择好的东西放到pool中</p>
<p>域内的表现不错，但是还可以加上聚合来加强表现。</p>
<p>原本的指标（metric）是accuracy和log-likelihood。但是LL可能会误把好模型忽略（因为刚好无法校准）但可以用温度缩放来调整回来。cLL可以得到更好的M</p>
<p>领域转移的健壮性：CIFAR10-C, CIFAR-100-C and ImageNet-C datasets with 15 corruptions C from groups noise, blur, weather and digital.（数据）对每个确定的level都计算错误率，对每个corruption都计算平均</p>
<p>一般来看，在干净的验证数据上训练的策略对损坏的数据有很好的效果。（换句话说，在哪里GPS效果都很好） transfer GPS policy 效果在不同的数据集上不错</p>
<p>结合聚合方法效果也不错</p>
<p>在原始的数据增广上也有不错的表现</p>
<p>未加valid 5000</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image006-1736691190944-3.png)</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image008-1736691190944-4.png)</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image010-1736691190944-5.png)</p>
<p>校准之后的数据</p>
<p>加了valid5000</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image012.jpg)</p>
<p>训练randaugment</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image014-1736691190944-6.png)</p>
<p>第一次增广预测</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image016-1736691190944-8.png)</p>
<p>第二次增广预测</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image018.png)</p>
<p>第三次增广预测</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image020-1736691190944-7.png)</p>
<p>第四次增广预测</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image022.png)</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image024-1736691190945-9.png)</p>
<p>GPS过程</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image026-1736691190945-10.png)评价过程</p>
<p>Reference：</p>
<p><a href="https://zhuanlan.zhihu.com/p/620688513">Pytorch格式 .pt .pth .bin .onnx 详解 - 知乎 (zhihu.com)</a></p>
<p><a href="https://blog.csdn.net/m0_51004308/article/details/118001835">Pytorch学习笔记(七):F.softmax()和F.log_softmax函数详解-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/qiankendeNMY/article/details/126508781?spm=1018.2226.3001.9630.1&extra%5Btitle%5D=Python%E2%80%94argparse%E6%A8%A1%E5%9D%97&extra%5Butm_source%5D=vip_chatgpt_common_pc_toolbar&extra%5Butm_medium%5D=">Python—argparse模块_parser.add_argument(‘–data’, type&#x3D;str, default&#x3D;’’-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/tsinghuahui/article/details/89279152">python argparse中action的可选参数store_true的作用_argparse store_true-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/u010804417/article/details/92843528">伪代码书写规则_伪代码怎么写-CSDN博客</a>（在伪码中，符号∅通常表示空集或空值）</p>
<p><a href="https://zhuanlan.zhihu.com/p/26614750">一文搞懂极大似然估计 - 知乎 (zhihu.com)</a></p>
<p><a href="https://blog.csdn.net/Elenstone/article/details/105677886">机器学习最易懂之贝叶斯模型详解与python实现_pytorch实现贝叶斯决策模型-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/qq_23933415/article/details/111212898">模型的参数verbose用法详解_verbose参数-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/weixin_44912159/article/details/104921422">论文中的常见缩写(w.r.t&#x2F;i.e.&#x2F;et al等)的意思-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/fengbingchun/article/details/124648766">深度学习中的优化算法之带Momentum的SGD_momentum sgd-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/wangxuecheng666/article/details/109246126">StratifiedShuffleSplit（）函数的详细理解-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/qq_37297763/article/details/116847455">Pytorch学习（十三）python中*args和**kwargs的用法_pytorch args.-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/Just_do_myself/article/details/123751148?spm=1018.2226.3001.9630.1&extra%5Btitle%5D=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8A%A8%E6%80%81%E8%B0%83%E6%95%B4%E5%AD%A6%E4%B9%A0%E7%8E%87LR&extra%5Butm_source%5D=vip_chatgpt_common_pc_toolbar&extra%5Butm_medium%5D=">深度学习之动态调整学习率LR_动态学习率-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/qq_42079689/article/details/102873766">PyTorch的nn.Linear（）详解_nn.linear()-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/weixin_42843425/article/details/122518502">model.parameters()的理解与使用-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/qq_41813454/article/details/135129279?spm=1018.2226.3001.9630.1&extra%5Btitle%5D=%E5%88%AB%E5%86%8D%E6%B7%B7%E6%B7%86%E4%BA%86%EF%BC%81model.eval()%E5%92%8Ctorch.no_grad()%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%80%E6%AC%A1%E8%AE%B2%E6%B8%85%E6%A5%9A&extra%5Butm_source%5D=vip_chatgpt_common_pc_toolbar&extra%5Butm_medium%5D=">别再混淆了！model.eval()和torch.no_grad()的区别一次讲清楚-CSDN博客</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/38121870">机器学习之K折交叉验证 - 知乎 (zhihu.com)</a></p>
<p><a href="https://blog.csdn.net/qq_39478403/article/details/121107057">【机器学习】浅谈 归纳偏置 (Inductive Bias)-CSDN博客</a></p>
<p>· store_true 是指带触发 action 时为真，不触发则为假， 即默认 False ，传参 则 设置为 True<br> · store_false 则与之相反</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image028-1736691190945-11.png)![img](D:\Program Files (x86)\Typora\img-data\clip_image030-1736691190945-12.png)![img](D:\Program Files (x86)\Typora\img-data\clip_image032-1736691190945-13.png)</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image034-1736691190945-14.png)![img](D:\Program Files (x86)\Typora\img-data\clip_image036-1736691190945-16.png)</p>
<p>enumerate函数，它可以同时返回列表中的元素和它们的索引</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image038-1736691190945-15.png)</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image040-1736691190945-18.png)</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image042.png)</p>
<p>![img](D:\Program Files (x86)\Typora\img-data\clip_image044.png)</p>
<p>ablation study (消融实验）</p>
<p><a href="https://zhuanlan.zhihu.com/p/644502891">一文搞懂什么是ablation study (消融实验） - 知乎 (zhihu.com)</a><br> adversarial attacks 对抗攻击 在某些微小影响下会导致巨大偏差</p>
<p><a href="https://zhuanlan.zhihu.com/p/104532285">CV||对抗攻击领域综述（adversarial attack） - 知乎 (zhihu.com)</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/293065153">Google开箱即用数据增强RandAugment | NeurIPS 2020 - 知乎 (zhihu.com)</a></p>
<p>原始的自动数据增强方式，先在一个单独的小数据集代理任务（可以理解成小&#x2F;子数据集，子任务 ）上进行，然后再迁移到更大数据的目标任务中去。作者提出了一种实用的自动数据增强算法RandAugment。该方法无需在子任务上进行搜索验证。但为了减少数据增强的参数搜索空间，作者设计了一种简单的格点搜索（simple grid search）方法以学习&#x2F;获取一种数据增强的策略。<strong>RandAugment只有两个参数：N和M。 其中N为在每次增强时使用N次操作（使用的这N个操作，都是从操作集中等概率抽取的，例如操作集中有14种操作，则每种操作被选中的概率为1&#x2F;14，每张图像的N次增强中，选到的操作可能是一样的），M为正整数，表示所有操作在应用时，幅度都为M（旋转多少）</strong>。</p>
<p><strong>用于数据增广</strong>![img](D:\Program Files (x86)\Typora\img-data\clip_image046-1736691190945-17.png)</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>校准</tag>
      </tags>
  </entry>
</search>
